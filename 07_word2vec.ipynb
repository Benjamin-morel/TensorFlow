{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNd9N+jMHq7cfvWch2LbV+1",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Benjamin-morel/TensorFlow/blob/main/07_word2vec.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "\n",
        "\n",
        "# **Machine Learning Model: word2vec and skip-gram**\n",
        "\n",
        "| | |\n",
        "|------|------|\n",
        "| Filename | 07_word2vec.ipynb |\n",
        "| Author(s) | Benjamin Morel (benjaminmorel27@gmail.com) |\n",
        "| Date | January 26, 2024 |\n",
        "| Aim(s) | build a word embedding space |\n",
        "| Dataset(s) | IMDb Movie Reviews dataset [[1]](https://aclanthology.org/P11-1015.pdf)|\n",
        "| Version | Python 3.10.12 - TensorFlow 2.17.1 |\n",
        "\n",
        "\n",
        "<br> **!!Read before running!!** <br>\n",
        "1. Fill in the inputs\n",
        "2. CPU execution is enough\n",
        "3. Run all and read comments\n",
        "\n",
        "---\n",
        "\n",
        "#### **Motivation**\n",
        "\n",
        "Using the IMDb dataset used in code [02_classfication_text.ipynb](https://github.com/Benjamin-morel/TensorFlow/tree/main), the model is trained using the word2vec technique and model's weights are then used to construct a word embedding space.  \n",
        "\n",
        "#### **Outline**\n",
        "\n",
        "\n",
        "\n",
        "*   retrieve text data & pre-processing\n",
        "*   training data generation\n",
        "*   training phase\n",
        "*   exploration of the embedding space\n",
        "*   conclusion and comparison\n",
        "*   references\n",
        "\n",
        "---"
      ],
      "metadata": {
        "id": "IKO5J-_-Vj3U"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **0. Input section**"
      ],
      "metadata": {
        "id": "an7V4rZe30MH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The model has already been trained and the user can choose to used a pre-trained model (No) or to repeat the training phase (Yes). Using a pre-trained model saves time, computer resources and CO2 emissions."
      ],
      "metadata": {
        "id": "LfclVMP737Ey"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "training_phase = \"No\""
      ],
      "metadata": {
        "id": "JvszvjSG37h-"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n",
        "## **1. Python librairies & display utilities**"
      ],
      "metadata": {
        "id": "iOr8jVUqvebl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# @title 1.1. Python librairies [RUN ME]\n",
        "\n",
        "\"\"\" math \"\"\"\n",
        "import numpy as np # linear algebra\n",
        "\n",
        "\"\"\" file opening and pre-processing\"\"\"\n",
        "import os # miscellaneous operating system interfaces\n",
        "import pandas as pd # data manipulation tool\n",
        "from re import escape # regular expressions\n",
        "import string # string manipulation\n",
        "\n",
        "\"\"\" ML models \"\"\"\n",
        "import tensorflow as tf # framework for ML/DL\n",
        "from tensorflow import keras # API used to build model in TensorFlow\n",
        "\n",
        "\"\"\" exportation \"\"\"\n",
        "import pickle # serialization"
      ],
      "metadata": {
        "cellView": "form",
        "id": "Rc2oQxKkvr-C"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title 1.2. Import Github files [RUN ME]\n",
        "\n",
        "\"\"\"Clone the Github repertory TensorFlow and imports the files required (see section 3.2)\"\"\"\n",
        "\n",
        "def get_github_files():\n",
        "  !git clone https://github.com/Benjamin-morel/TensorFlow.git TensorFlow_duplicata\n",
        "  path_model = 'TensorFlow_duplicata/99_pre_trained_models/07_word2vec/07_word2vec.keras'\n",
        "  path_dictionary = 'TensorFlow_duplicata/99_pre_trained_models/07_word2vec/dictionary_embedding.pkl'\n",
        "  model = keras.models.load_model(path_model, custom_objects={'Word2Vec': Word2Vec})\n",
        "  with open(path_dictionary, 'rb') as f:\n",
        "    dictionary = pickle.load(f)\n",
        "  !rm -rf TensorFlow_duplicata/\n",
        "  return model, dictionary"
      ],
      "metadata": {
        "cellView": "form",
        "id": "hJrHq8awwNgH"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "\n",
        "\n",
        "## **2. Data retrieval and set generation**"
      ],
      "metadata": {
        "id": "vTJzD6Tv4Fca"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The skip-gram method is a word embedding technique used in Word2Vec to learn vector representations of words based on their surrounding context. It works by selecting a target word and predicting its context words within a given window size.\n",
        "\n",
        ">Example: \"the dog barks loudly\"\n",
        "\n",
        ">Window = 1 <br>\n",
        ">Target word: \"dog\" <br>\n",
        ">Context words: \"the\" and \"barks\"\n",
        "\n",
        ">Window = 2 <br>\n",
        ">Target word: \"dog\" <br>\n",
        ">Context words: \"the\" and \"barks\" and \"loudly\"\n",
        "\n",
        "Each word is converted to an integer and passed through a neural network with a single hidden layer. The model is trained to maximize the probability of context words given the target word, often using negative sampling to improve efficiency.\n",
        "\n",
        "After training, the hidden layer weights form a word embeddings space, where words are represented by a 16-dimensional embedding vector. Skip-gram is especially useful for capturing relationships between words.\n"
      ],
      "metadata": {
        "id": "DvEtr1KoFGLk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2.1. Retrieve data\n",
        "\n",
        "The IMDb Movie Reviews is a database created for sentiment analysis in movie reviews. It contains 50,000 movie reviews. The database is extracted and placed in the folder review_dataset. The architecture of review_dataset is as follows:\n",
        "\n",
        "```markdown\n",
        "**Imdb_dataset/**\n",
        ". . . aclImdb/\n",
        ". . . . . . train/\n",
        ". . . . . . test/\n",
        ". . . . . . README\n",
        ". . . . . . imdb.vocab\n",
        ". . . . . . imdbEr.text\n",
        "```\n",
        "\n",
        "Other files are included in the folder like `README` which provides information about the dataset and how to use it. Files `imdb.vocab` and `imdbEr.txt` contain additional information about errors, URL website and specific annotations.\n",
        "\n",
        "To train the model, the labels used to classify are not required."
      ],
      "metadata": {
        "id": "CCedTxGVU898"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_data(url, batch_size):\n",
        "\n",
        "  dataset_name = \"Imdb_dataset_1\"\n",
        "  path = tf.keras.utils.get_file(dataset_name, url, extract=True)\n",
        "  path = os.path.join(path, 'aclImdb')\n",
        "  train_path = os.path.join(path, 'train')\n",
        "  test_path = os.path.join(path, 'test')\n",
        "\n",
        "  raw_train_ds = tf.keras.utils.text_dataset_from_directory(train_path, labels=None, batch_size=batch_size, validation_split=None, shuffle=True, seed=1, verbose=0)\n",
        "  raw_test_ds = tf.keras.utils.text_dataset_from_directory(test_path, labels=None, batch_size=batch_size, verbose=0)\n",
        "\n",
        "  return raw_train_ds, raw_test_ds"
      ],
      "metadata": {
        "id": "6tyDasAh_ZyY"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "url = \"https://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz\"\n",
        "\n",
        "batch_size = 1\n",
        "\n",
        "train_ds, test_ds = get_data(url, batch_size)"
      ],
      "metadata": {
        "id": "sVSYard4_kii"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The batch size is 1 to facilitate pre-processing of texts. The variable `nb_text` is used to control the number of texts to be supplied to the model.\n",
        "\n"
      ],
      "metadata": {
        "id": "d5rIxD1iGh-q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "text_nb = 25000\n",
        "train_ds = train_ds.take(text_nb)"
      ],
      "metadata": {
        "id": "K9xSdAZOT2JD"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for text in train_ds.take(1):\n",
        "  print(text.numpy())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-0h9KWKNG8Y3",
        "outputId": "f5fe7267-effe-497a-d8c2-8bc28932953c"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[b\"Who actually created this piece of crap this is the worst movie i have ever seen in my life it is such a waste of time and money. I hate it how they create low budget sequels featuring D-Lister actors and a storyline so similar to the 1st one.<br /><br />I found this movie in the bargain bin sitting right next to Wild Things 2 and Death To The Supermodels for $2.99 what a fool i was to actually think that this could be good instead i watched in disgust as poor acting stereotypes ripped of the storyline and script from the 1st one.<br /><br />Whoever thought that this straight-to-video production was actually even a half decent film you must be on crackd or something because I think what pretty much most of the people who've seen this film thinks WHAT A LOAD OF CRAP!!!!\"]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2.2. Pre-processing\n",
        "\n",
        "Input data must be processed:\n",
        "```markdown\n",
        "Tensor(['The film was ....'])\n",
        "Tensor(['I see this movie ...'])\n",
        ". . .\n",
        "```\n",
        "Each tensor is composed of a sentence (and not an entire movie review). So, for one review, several tensors can be generated. It is assumed that each sentence carries a context/meaning. A sentence is defined here as a series of words ending by a `. `, a `? ` or a ' ! `."
      ],
      "metadata": {
        "id": "t7dr6A2lWIJS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "remove_ponctuation = escape(string.punctuation).replace('.', '').replace('?', '').replace('!', '')"
      ],
      "metadata": {
        "id": "2HK0RxBRxaKr"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The 50 most frequent words in the dataset are also removed. This list of words comes from the Jupyter Notebook [02_classfication_text.ipynb](https://github.com/Benjamin-morel/TensorFlow/tree/main). These words are removed because they carry a little meaning and increase the size of the model's input vectors:\n",
        "\n",
        "```markdown\n",
        "The dog barked loudly --> dog barked loudly --> ['dog', 'barked', 'loudly']\n",
        "```"
      ],
      "metadata": {
        "id": "Hcbf6V3WQNkC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "common_token = [' the ', ' and ', ' a ', ' of ', ' to ', ' is ', ' in ', ' it ', ' i ', ' this ', ' that ', ' br ', ' was ', ' as ', ' with ', ' for ',\n",
        "                ' you ', ' on ', ' are ', ' one ', ' be ', ' he ', ' its ', ' have ', ' an ', ' by ', ' at ', ' all ', ' from ', ' who ', ' so ',\n",
        "                ' they ', ' her ', ' just ', ' some ', ' out ', ' about ', ' or ', ' s ', ' if ', '  c  ', ' there ', ' were ', ' would ', ' had ', ' it ', ' we ']"
      ],
      "metadata": {
        "id": "ZCDiZgqznO3T"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def custom_standardization(input_text):\n",
        "  text_modified = tf.strings.lower(input_text) # upper cases --> lower cases\n",
        "  text_modified = tf.strings.regex_replace(text_modified, '<br />', ' ') # remove HTML strings\n",
        "  for i in range(len(common_token)):\n",
        "    text_modified = tf.strings.regex_replace(text_modified, common_token[i], ' ') # remove frequent words\n",
        "  text_modified = tf.strings.regex_replace(text_modified, '[%s]' % remove_ponctuation, '') # remove punctuation\n",
        "  return text_modified"
      ],
      "metadata": {
        "id": "WYbUfk3h31Ow"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_ds = train_ds.map(lambda x: custom_standardization(x))"
      ],
      "metadata": {
        "id": "wS7XzSSMxj15"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "An example of a pre-processed movie review is displayed. Only the punctuation elements `.`, `?` and `!` are retained."
      ],
      "metadata": {
        "id": "87oEZsURRan_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for text in train_ds.take(1):\n",
        "  print(text.numpy())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9OKNwgvsxpPU",
        "outputId": "95ee49d2-d52d-4c0d-c487-2d3eb6258192"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[b'nothing can prepare another lousy bimbo outing! time its being brought neverinevitable fred olen ray! far exploitation movies go doesnt click! science fiction its plain unoriginal! see an ugly feminine android wearing bikini destroy earth showing off thats nearly bare resist! give me fing break!!! kind entertainment your thing then why not dust off those old si swimsuit mags attic change?! been much better didnt set sleaze factor very high but still wouldnt make great. id like point another film called assault 1996 jim wynorski which resembles identity alienator. illustrates why topnotch 1stperson femme fatale action movies dont translate well america. sorry fellas!']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "From these pre-processed texts, the sentences are separated and isolated into tensors."
      ],
      "metadata": {
        "id": "rQ3DJ4c6SJfA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def split_sentences(text):\n",
        "    text =  tf.strings.split(text, '. ')\n",
        "    text =  tf.strings.split(text, '? ')\n",
        "    text =  tf.strings.split(text, '! ')\n",
        "    return text"
      ],
      "metadata": {
        "id": "d-xpMZykxvWQ"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_ds = train_ds.map(lambda x: split_sentences(x))"
      ],
      "metadata": {
        "id": "pkgs2qz-xx-2"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Finally, the tensors contained in `train_ds` - with variable size - are converted into individual scalar tensors."
      ],
      "metadata": {
        "id": "giPPN4gmULcU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def split_ragged_tensor(ragged_tensor):\n",
        "    flat_values = ragged_tensor.flat_values\n",
        "    return tf.data.Dataset.from_tensor_slices(flat_values)"
      ],
      "metadata": {
        "id": "4OqamkAlyKxb"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_ds = train_ds.flat_map(split_ragged_tensor)"
      ],
      "metadata": {
        "id": "sGE9NNatyMbM"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for text in train_ds.take(5):\n",
        "  print(text.numpy())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "P2l_6omwSaQa",
        "outputId": "e9d8985d-c3cb-4c07-a26a-977bf9fb5df9"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "b'this movie not great.i found story too banalordinary.theres not much originality here.its combination many other movies.its equal parts christmas carolits wonderful lifehow grinch stole christmasand even cat hat movie.the movie isnt very funnybut bit slapstick works.this movie isi feltoverly sentimental preachy.in facti felt like watching 90 minute commercial how important family is.nowdont get me wrong.family very important.i find subtlety works best these movies.this way too heavy handed me.but good news.the movie has great visual style.i meanit looks fantastically magical.and martin short terrific jack frostthe baddie piece.hes not really scarymore mildly disconcerting than anythingand even bit sad.i also like look gave him.this movie also bit tearjerker.anywaythis case style over substance.and while its not nearly mean spirited creepy part 2i still dont think good.the negatives outweigh positives.for methe santa clause 3 410'\n",
            "b'without doubt my absolute favorite film time'\n",
            "b'first saw movie three years ago been love and stanley kubrick ever since'\n",
            "b'never get tired seeing movie'\n",
            "b'why remains underappreciated at least casual movie viewers beyond me'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2.3. Vectorization\n",
        "\n",
        "A maximum of 20 tokens are generated for each sentence and then vectorized."
      ],
      "metadata": {
        "id": "gO0xPfviSlL2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "max_features = 5000\n",
        "max_length = 20\n",
        "\n",
        "vectorize_layer = tf.keras.layers.TextVectorization(standardize=None, # already done\n",
        "                                                    max_tokens=max_features,\n",
        "                                                    output_mode='int',\n",
        "                                                    output_sequence_length=max_length)"
      ],
      "metadata": {
        "id": "JKsOEFqVWFBL"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "vectorize_layer.adapt(train_ds)"
      ],
      "metadata": {
        "id": "0mNNDtBL32_P"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_ds = train_ds.map(vectorize_layer)"
      ],
      "metadata": {
        "id": "N0E_KWdKzZOL"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "word_list = vectorize_layer.get_vocabulary()\n",
        "int_list = list(train_ds.as_numpy_iterator())\n",
        "\n",
        "for seq in int_list[:5]:\n",
        "  print(f\"{seq} => {[word_list[i] for i in seq]}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "mVYcxH6OznQJ",
        "outputId": "44bcac8d-0b92-4c49-f887-3d274bfed249"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[ 63  43 969  58  82   1 655  22   0   0   0   0   0   0   0   0   0   0\n",
            "   0   0] => ['this', 'first', 'creepy', 'movies', 'ever', '[UNK]', '5', 'time', '', '', '', '', '', '', '', '', '', '', '', '']\n",
            "[1690   29   11    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0] => ['scared', 'me', 'good', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '']\n",
            "[   3  269  237    1    1  868    7    1   19 1555  142   14 2904    0\n",
            "    0    0    0    0    0    0] => ['but', 'night', 'put', '[UNK]', '[UNK]', 'eye', 'like', '[UNK]', 'my', 'mom', 'got', 'very', 'upset', '', '', '', '', '', '', '']\n",
            "[  17 2320   19  868 4963  305  227   19  868    1    7 1634    1    0\n",
            "    0    0    0    0    0    0] => ['she', 'clean', 'my', 'eye', 'alcohol', 'next', 'day', 'my', 'eye', '[UNK]', 'like', 'double', '[UNK]', '', '', '', '', '', '', '']\n",
            "[112 134   2   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
            "   0   0] => ['now', 'thats', 'movie', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "\n",
        "\n",
        "## **3. Training set generation**"
      ],
      "metadata": {
        "id": "shZUw7pEXayn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3.1. Skip-gram pair generation\n",
        "\n",
        "Generates skip-gram pairs with negative sampling for a list of sequences (int-encoded sentences) based on window size, number of negative samples and vocabulary size."
      ],
      "metadata": {
        "id": "kbDFjB9AX9nR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_training_data(sequences, window_size, num_ns, vocab_size, seed):\n",
        "  targets, contexts, labels = [], [], []\n",
        "\n",
        "  sampling_table = tf.keras.preprocessing.sequence.make_sampling_table(vocab_size) # Build the sampling table for `vocab_size` tokens.\n",
        "\n",
        "  for sequence in sequences: # Iterate over all sequences (sentences) in the dataset\n",
        "\n",
        "    positive_skip_grams, _ = tf.keras.preprocessing.sequence.skipgrams(\n",
        "          sequence,\n",
        "          vocabulary_size=vocab_size,\n",
        "          sampling_table=sampling_table,\n",
        "          window_size=window_size,\n",
        "          negative_samples=0) # Generate positive skip-gram pairs for a sequence (sentence)\n",
        "\n",
        "    for target_word, context_word in positive_skip_grams: # Iterate over each positive skip-gram pair to produce training examples with a positive context word and negative samples.\n",
        "      context_class = tf.expand_dims(\n",
        "          tf.constant([context_word], dtype=\"int64\"), 1)\n",
        "      negative_sampling_candidates, _, _ = tf.random.log_uniform_candidate_sampler(\n",
        "          true_classes=context_class,\n",
        "          num_true=1,\n",
        "          num_sampled=num_ns,\n",
        "          unique=True,\n",
        "          range_max=vocab_size,\n",
        "          seed=seed,\n",
        "          name=\"negative_sampling\")\n",
        "\n",
        "\n",
        "      context = tf.concat([tf.squeeze(context_class,1), negative_sampling_candidates], 0) # Build context and label vectors (for one target word)\n",
        "      label = tf.constant([1] + [0]*num_ns, dtype=\"int64\")\n",
        "\n",
        "      targets.append(target_word) # Append each element from the training example to global lists.\n",
        "      contexts.append(context)\n",
        "      labels.append(label)\n",
        "\n",
        "  return targets, contexts, labels"
      ],
      "metadata": {
        "id": "QA4gyGLV3qak"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if training_phase == \"Yes\":\n",
        "  targets, contexts, labels = generate_training_data(sequences=int_list,\n",
        "                                                    window_size=2,\n",
        "                                                    num_ns=4,\n",
        "                                                    vocab_size=max_features,\n",
        "                                                    seed=1)\n",
        "\n",
        "  targets = np.array(targets)\n",
        "  contexts = np.array(contexts)\n",
        "  labels = np.array(labels)\n",
        "\n",
        "  print(f\"targets.shape: {targets.shape}\")\n",
        "  print(f\"contexts.shape: {contexts.shape}\")\n",
        "  print(f\"labels.shape: {labels.shape}\")"
      ],
      "metadata": {
        "id": "T_fLkogA3-gF"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3.2. Performances and batches"
      ],
      "metadata": {
        "id": "UgY7QoLBYBrf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "if training_phase == \"Yes\":\n",
        "  batch_size = 1024\n",
        "  AUTOTUNE = tf.data.AUTOTUNE\n",
        "  dataset = tf.data.Dataset.from_tensor_slices(((targets, contexts), labels))\n",
        "  dataset = dataset.shuffle(len(targets)).batch(batch_size).repeat()\n",
        "\n",
        "  dataset = dataset.cache().prefetch(buffer_size=AUTOTUNE)"
      ],
      "metadata": {
        "id": "8NoVgzD-KC-S"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3.3. Model definition"
      ],
      "metadata": {
        "id": "tiVadQe9YHMm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "@keras.utils.register_keras_serializable()\n",
        "class Word2Vec(tf.keras.Model):\n",
        "  def __init__(self, vocab_size, embedding_dim, **kwargs):\n",
        "    super(Word2Vec, self).__init__()\n",
        "    self.vocab_size = vocab_size # Store vocab_size as an attribute\n",
        "    self.embedding_dim = embedding_dim # Store embedding_dim as an attribute\n",
        "    self.target_embedding = tf.keras.layers.Embedding(vocab_size,\n",
        "                                      embedding_dim,\n",
        "                                      name=\"w2v_embedding\")\n",
        "    self.context_embedding = tf.keras.layers.Embedding(vocab_size,\n",
        "                                       embedding_dim)\n",
        "\n",
        "  def call(self, pair):\n",
        "    target, context = pair\n",
        "    # target: (batch, dummy?)  # The dummy axis doesn't exist in TF2.7+\n",
        "    # context: (batch, context)\n",
        "    if len(target.shape) == 2:\n",
        "      target = tf.squeeze(target, axis=1)\n",
        "    # target: (batch,)\n",
        "    word_emb = self.target_embedding(target)\n",
        "    # word_emb: (batch, embed)\n",
        "    context_emb = self.context_embedding(context)\n",
        "    # context_emb: (batch, context, embed)\n",
        "    dots = tf.einsum('be,bce->bc', word_emb, context_emb)\n",
        "    # dots: (batch, context)\n",
        "    return dots\n",
        "\n",
        "  def get_config(self): # Define get_config to include vocab_size and embedding_dim\n",
        "    config = super(Word2Vec, self).get_config()\n",
        "    config.update({\"vocab_size\": self.vocab_size, \"embedding_dim\": self.embedding_dim})\n",
        "    return config\n",
        "\n",
        "  @classmethod\n",
        "  def from_config(cls, config): # Define from_config to use vocab_size and embedding_dim\n",
        "      return cls(**config)"
      ],
      "metadata": {
        "id": "-Fmo3B7x4Ebi"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def create_model(embedding_dim):\n",
        "  model = Word2Vec(max_features, embedding_dim)\n",
        "  model.compile(optimizer='adam',\n",
        "                loss=tf.keras.losses.CategoricalCrossentropy(from_logits=True),\n",
        "                metrics=None)\n",
        "  return model"
      ],
      "metadata": {
        "id": "_AUxMqyq9jq3"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3.4. Training phase"
      ],
      "metadata": {
        "id": "_-D7FGkZYKg0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def train_model(model, training_data, steps_per_epoch, **kwargs):\n",
        "  kwargs.setdefault(\"epochs\", 20)\n",
        "  kwargs.setdefault(\"verbose\", 2)\n",
        "  log = model.fit(training_data, steps_per_epoch = steps_per_epoch, **kwargs)\n",
        "\n",
        "  return log.history[\"loss\"]"
      ],
      "metadata": {
        "id": "MmaQTLUD8bE0"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if training_phase == \"Yes\":\n",
        "  model = create_model(16)\n",
        "  steps_per_epoch = len(targets) // batch_size\n",
        "  history = train_model(model, dataset, steps_per_epoch)\n",
        "  model.save('07_word2vec.keras')\n",
        "else:\n",
        "  model, dictionary_embedding = get_github_files()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WxdU_xWo-Fpk",
        "outputId": "0892894e-1780-4282-827d-d7ac4284a6ac"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'TensorFlow_duplicata'...\n",
            "remote: Enumerating objects: 655, done.\u001b[K\n",
            "remote: Counting objects: 100% (287/287), done.\u001b[K\n",
            "remote: Compressing objects: 100% (193/193), done.\u001b[K\n",
            "remote: Total 655 (delta 197), reused 94 (delta 94), pack-reused 368 (from 1)\u001b[K\n",
            "Receiving objects: 100% (655/655), 174.58 MiB | 21.22 MiB/s, done.\n",
            "Resolving deltas: 100% (324/324), done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "\n",
        "\n",
        "## **4. Embedding space exploration**"
      ],
      "metadata": {
        "id": "O2ayOZiwYfeN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 4.1. Embedding dictionary"
      ],
      "metadata": {
        "id": "h2ApzsXuYoie"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "weights = model.get_layer('w2v_embedding').get_weights()[0]\n",
        "vocab = vectorize_layer.get_vocabulary()\n",
        "dictionary_embedding = {vectorize_layer.get_vocabulary()[i]:weights[i] for i in range(len(vocab))} # build the embedding dictionary\n",
        "\n",
        "with open('dictionary_embedding.pkl', 'wb') as f:\n",
        "  pickle.dump(dictionary_embedding, f)"
      ],
      "metadata": {
        "id": "KEAQX3iF4NDi"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dictionary_embedding[\"money\"]"
      ],
      "metadata": {
        "id": "UF-ehK1Lie7m",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "33d3c438-2111-4363-8467-3b92dc9f9179"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([-0.3503012 , -0.15039295, -0.32417077,  0.36491925, -0.39782122,\n",
              "        0.07117482,  0.4433823 , -0.30348516,  0.3206255 ,  0.2682087 ,\n",
              "        0.5566837 ,  0.6386071 , -0.45531398,  0.09227507,  0.39550662,\n",
              "        0.11250176], dtype=float32)"
            ]
          },
          "metadata": {},
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "with open('vocab.pkl', 'wb') as f:\n",
        "  pickle.dump(vocab, f)"
      ],
      "metadata": {
        "id": "k5V437ng4FsN"
      },
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "with open('vocab.pkl', 'rb') as f:\n",
        "  vocab = pickle.load(f)"
      ],
      "metadata": {
        "id": "zo4ntrAF4iCB"
      },
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 4.2. Similarities and analogies"
      ],
      "metadata": {
        "id": "jkHFz7FSYv5R"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# get the distance between two elements in the embedding space\n",
        "\n",
        "def get_distance(token1, token2):\n",
        "  p1 = dictionary_embedding[token1]\n",
        "  p2 = dictionary_embedding[token2]\n",
        "  distance = np.linalg.norm(p2-p1)\n",
        "  return distance\n",
        "\n",
        "# get the cosinus similarity between two elements in the embedding space\n",
        "\n",
        "def get_cosinus_similarity(token1, token2):\n",
        "  p1 = dictionary_embedding[token1]\n",
        "  p2 = dictionary_embedding[token2]\n",
        "  dot_product = np.dot(p1, p2)\n",
        "  magnitude_1 = np.linalg.norm(p1)\n",
        "  magnitude_2 = np.linalg.norm(p2)\n",
        "  cosine_sim = dot_product / (magnitude_1 * magnitude_2)\n",
        "  return cosine_sim"
      ],
      "metadata": {
        "id": "D32RiQSyihOX"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# get elements closest to a specific element in the embedding space\n",
        "\n",
        "def get_synomym(token, n, used_distance=True):\n",
        "  p1 = dictionary_embedding[token]\n",
        "  candidate_list = {} # stores n synonyms\n",
        "  for i in range(1, len(vocab)): # only the first 1000 words of the embedding dictionary are searched\n",
        "    token_candidate = vocab[i]\n",
        "    if used_distance == True: candidate_list[token_candidate] = get_distance(token, token_candidate)\n",
        "    else: candidate_list[token_candidate] = get_cosinus_similarity(token, token_candidate)\n",
        "\n",
        "  sorted_items = sorted(candidate_list.items(), key=lambda item: item[1])\n",
        "\n",
        "  if used_distance == True: synonym_list = sorted_items[1:n+1]\n",
        "  else: synonym_list = sorted_items[-(n+1):-1]\n",
        "  words = [(item[0], item[1]) for item in synonym_list]\n",
        "  return print(words)"
      ],
      "metadata": {
        "id": "FNWrbXIUiuI7"
      },
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "word = \"house\"\n",
        "nb = 10\n",
        "get_synomym(word, nb, used_distance=False)"
      ],
      "metadata": {
        "id": "8sUp8U2FiwUP",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3d8bf03f-3160-4cec-bcf7-e5326ddbf263"
      },
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[('security', 0.811774), ('island', 0.81794286), ('water', 0.8186376), ('kitchen', 0.82224846), ('office', 0.8228932), ('hotel', 0.8328615), ('search', 0.8471924), ('mountain', 0.84733915), ('apartment', 0.85803413), ('bed', 0.9000992)]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**\"terrible\"**\n",
        "\n",
        "*   awful - 0.9384527\n",
        "*   horrible - 0.8618901\n",
        "*   plain - 0.851014\n",
        "*   abysmal - 0.8413805\n",
        "*   uninspired - 0.83548206\n",
        "*   lame - 0.8295864\n",
        "*   unbelievably - 0.82898676\n",
        "*   dreadful - 0.8224774\n",
        "*   unwatchable - 0.81847626\n",
        "*   horrendous - 0.81711394\n",
        "\n",
        "**\"amazing\"**\n",
        "\n",
        "*   brilliant - 0.8560008\n",
        "*   understated - 0.8467297\n",
        "*   remarkable - 0.84653646\n",
        "*   outrageous - 0.8109178\n",
        "*   wonderful - 0.8065811\n",
        "*   delightful - 0.8046221\n",
        "*   exceptional - 0.80231535\n",
        "*   marvelous - 0.7814437\n",
        "*   accomplished - 0.78039724\n",
        "*   underrated - 0.78019947\n",
        "\n",
        "**\"house\"**\n",
        "\n",
        "*   bed - 0.9000992\n",
        "*   apartment - 0.85803413\n",
        "*   mountain - 0.84733915\n",
        "*   search - 0.8471924\n",
        "*   hotel - 0.8328615\n",
        "*   office - 0.8228932\n",
        "*   kitchen - 0.82224846\n",
        "*   water - 0.8186376\n",
        "*   island - 0.81794286\n",
        "*   security - 0.811774"
      ],
      "metadata": {
        "id": "e-gQeX0p96fW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_analogy(vector, n):\n",
        "  candidate_list = {} # stores n synonyms\n",
        "  for i in range(1, len(vocab)):\n",
        "    token_candidate = vocab[i]\n",
        "    vector_candidate = dictionary_embedding[token_candidate]\n",
        "    candidate_list[token_candidate] = sum(abs(vector - vector_candidate))\n",
        "\n",
        "  sorted_items = sorted(candidate_list.items(), key=lambda item: item[1])\n",
        "  synonym_list = sorted_items[0:n]\n",
        "  words = [item[0] for item in synonym_list]\n",
        "  print(words)"
      ],
      "metadata": {
        "id": "FlzOsPSGRUmY"
      },
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "analogy = dictionary_embedding[\"funny\"] - dictionary_embedding[\"nice\"] + dictionary_embedding[\"terrible\"]\n",
        "get_analogy(analogy, 5)"
      ],
      "metadata": {
        "id": "TvO-QKgeRWPa",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "95988f29-2a4d-4045-8909-f0f076239c83"
      },
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['terrible', 'awful', 'bad', 'judging', 'horrendous']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "funny-nice+terrible = ['terrible', 'awful', 'bad', 'judging', 'horrendous']"
      ],
      "metadata": {
        "id": "sMBvS_z676Ev"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "\n",
        "\n",
        "## **5. Conclusion**"
      ],
      "metadata": {
        "id": "yPl2WcSSrQ7Q"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "\n",
        "\n",
        "## **6. References**\n",
        "\n",
        "| | | | | |\n",
        "|------|------|------|------|------|\n",
        "| Index | Title | Author(s) | Type | Comments |\n",
        "|[[1]](https://aclanthology.org/P11-1015.pdf) | IMDB dataset | Andrew L. Maas & al | dataset & paper | - |\n",
        "|[[2]](https://www.tensorflow.org/tutorials/keras/text_classification) | Basic text classification | TensorFlow | dataset | - |\n",
        "|[[3]](https://www.tensorflow.org/guide/data_performance) | Better performance with the tf.data API | TensorFlow | Tutoriels | - |\n",
        "|[[4]](https://www.cs.toronto.edu/~lczhang/360/lec/w05/w2v.html) | Word2Vec and GloVe Vectors | Toronto university | website | - |"
      ],
      "metadata": {
        "id": "iA4CnxdVY7OH"
      }
    }
  ]
}