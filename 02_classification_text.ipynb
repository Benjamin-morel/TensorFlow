{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Benjamin-morel/TensorFlow/blob/main/02_classification_text.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V-HaWB3Owxi1"
      },
      "source": [
        "---\n",
        "\n",
        "\n",
        "# **Machine Learning Model: basic text classification**\n",
        "\n",
        "| | |\n",
        "|------|------|\n",
        "| Filename | 02_classification_test.ipynb |\n",
        "| Author(s) | Benjamin Morel (benjaminmorel27@gmail.com) |\n",
        "| Date | September 4, 2024 |\n",
        "| Aim(s) | Build, train and evaluate a neural network machine learning model that classifies movie reviews as positives or negatives. |\n",
        "| Dataset(s) | Stanford dataset [[1]](https://ai.stanford.edu/~amaas/data/sentiment/)|\n",
        "| Version | Python 3.12 - TensorFlow 2.17.0 |\n",
        "\n",
        "\n",
        "<br> **!!Read before running!!** <br>\n",
        "1. Fill in the inputs\n",
        "2. GPU execution recommended if `training_phase=\"Yes\"`.\n",
        "3. Run all and read comments.\n",
        "\n",
        "---\n",
        "\n",
        "#### **Motivation**\n",
        "\n",
        "AI is capable of managing and processing text made up of several thousand words, special characters and punctuation. In this Python code, the construction of the neural network and its optimization is similar to what has been done before. The main interest lies in the way the text is processed and broken down into elements.\n",
        "\n",
        "For this, the Stanford Sentiment Treebank (SST) database, composed of over 50,000 movie reviews on the Internet, is used to build a binary classification neural network. A film review with a rating below 4/10 will be classified as negative and with a rating above 7/10 as positive.\n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GVXaEQPzZeq0"
      },
      "source": [
        "#### **0. Input section**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OVCvwvCvZgIJ"
      },
      "source": [
        "The model has already been trained: **parameters** (weights and biases) of each neuron are already known according to the base dataset. The user can choose to keep these parameters and **not retrain the model** (No), or he can decide to repeat the **training phase** (Yes). The latter choice may be justified by the fact that the user wishes to update the neural network against an updated dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Tla0F9fnZniD"
      },
      "outputs": [],
      "source": [
        "training_phase = 'No'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PNcpWfJ-9oED"
      },
      "source": [
        "---\n",
        "\n",
        "\n",
        "#### **1. Import libraries & prebuilt dataset**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_eE13ncCBllK"
      },
      "source": [
        "###### **1.1. Presentation of Python libraries**\n",
        "\n",
        "`os`: provides functions to interact with the operating system (manipulating files and directories, accessing system information...)\n",
        "\n",
        "`re`: performs complex searches and manipulations on strings (extracting substrings...)\n",
        "\n",
        "`shutil`: provides utilities for performing operations on files and directories (copying, modifying, deleting...)\n",
        "\n",
        "`string`: provides tools for handling and processing strings\n",
        "\n",
        "`numpy`: famous library for scientific computing\n",
        "\n",
        "`tensorflow`: builds and trains machine learning and deep learning models\n",
        "\n",
        "`plotly.express`: creates graphs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AYUJBo2iwkEH"
      },
      "outputs": [],
      "source": [
        "import os # miscellaneous operating system interfaces\n",
        "import re # regular expressions\n",
        "import shutil # operations on files\n",
        "import string # for manipulating strings\n",
        "import numpy as np # scientific computing\n",
        "import tensorflow as tf # machine learning models\n",
        "import plotly.express as px # graphing packages"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f2zcHbIgB3ql"
      },
      "source": [
        "###### **1.2. Which database is used?**\n",
        "\n",
        "The considered database is a version of the Stanford Sentiment Treebank (SST), a dataset created for sentiment analysis in movie reviews. It contains movie reviews labeled with sentiment (positive or negative) and is widely used to train and test sentiment analysis models. Movie reviews are contained into the compressed file `aclimdb`.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pZREb5FsxYCp"
      },
      "outputs": [],
      "source": [
        "url = \"https://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz\"\n",
        "\n",
        "dataset = tf.keras.utils.get_file(\"aclImdb_v1\", url, extract=True, cache_dir='.', cache_subdir='')\n",
        "dataset_dir = os.path.join(os.path.dirname(dataset), 'aclImdb')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9-rQAmrHV24_"
      },
      "source": [
        "###### **1.3. How data is organized within Stanford movie review database?**\n",
        "\n",
        "Once the file is extracted, the following structure is composed of 5 folders: `train`, `test`, `README`, `imdbEr.txt` and `imdb.vocab`. The following code line is used to check file name into the directory.\n",
        "\n",
        "*   `train`: contains movie reviews meant for training\n",
        "*   `test`: contains movie reviews meant for testing\n",
        "*   `README`: provides information about the dataset and how to use it\n",
        "*   `imdb.vocab` and `imdbEr.txt` contain additional information about errors, URL website and specific annotations\n",
        "\n",
        "Within both the `test` and `train` folders, there are two subfolders:\n",
        "\n",
        "*   `pos`: contains movie reviews with a positive sentiment (rating > 7/10)\n",
        "*   `unsup`: contains unlabeled movie reviews for unsupervised learning\n",
        "*   `neg`: contains movie reviews with a negative sentiment (rating < 4/10)\n",
        "\n",
        "Each text file in these subfolders represents a single movie review and contains the raw text of the review. They are cleaned to contain only raw text, without additional metadata."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yaYwjEYQAYJj",
        "outputId": "7a6a8e13-341d-41aa-88f3-09fb34fd23ea"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['test', 'imdb.vocab', 'README', 'imdbEr.txt', 'train']"
            ]
          },
          "metadata": {},
          "execution_count": 60
        }
      ],
      "source": [
        "os.listdir(dataset_dir) # check the file names in the aclImdb directory"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wJLocve1DXls"
      },
      "outputs": [],
      "source": [
        "train_dir = os.path.join(dataset_dir, 'train') # path name of the \"train\" file in dataset_dir\n",
        "remove_dir = os.path.join(train_dir, 'unsup') # remove the folder with unlabeled reviews for unsupervised learning\n",
        "shutil.rmtree(remove_dir)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i0TM938pDspy"
      },
      "source": [
        "###### **1.4. How data is submitted during the learning phase**\n",
        "\n",
        "A good practice for a machine learning experiment is to divide the dataset into 3 splits: `train`, `test` and `validation`. Two of them are already available. The validation set is created by using 20% of the training data set.\n",
        "\n",
        "The 3 subsets are organized in batches for multiple reasons: memory, parallel computation, fast calculation of gradients, standardize the processing of training, validation and test sets... For this, the TensorFlow function `text_dataset_from_directory` is used to randomly shuffle texts present in the train folder and then divide this shuffle to generate the training dataset (80%) and the validation dataset (20%). Finally, for each dataset, all the data is grouped into batches of 32 texts.\n",
        "\n",
        "The 3 subsets are stored in the data structures `tf.data.Dataset`. The data is not loaded into immediate memory, but generated when it is called up. The `show_text()` function displays an example of the text in the first batch of the data structure `tf.data.Dataset`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JNMXUoFwPfWv"
      },
      "outputs": [],
      "source": [
        "def show_text(text_batch): # get the first text of the first batch\n",
        "  for text_batch, label_batch in text_batch: # navigate through the text batch\n",
        "      if label_batch.numpy()[0] == 0:\n",
        "        print(\"Here's an extract from a negative review:\")\n",
        "        print(text_batch.numpy()[0])\n",
        "      else:\n",
        "        print(\"Here's an extract from a positive review:\")\n",
        "        print(text_batch.numpy()[0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5I8TejG3xtfj",
        "outputId": "45a7e7c4-977f-4cfb-af7e-e93916f49e2d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 25000 files belonging to 2 classes.\n",
            "Using 20000 files for training.\n",
            "Found 25000 files belonging to 2 classes.\n",
            "Using 5000 files for validation.\n",
            "Found 25000 files belonging to 2 classes.\n"
          ]
        }
      ],
      "source": [
        "batch_size = 32\n",
        "raw_train_ds = tf.keras.utils.text_dataset_from_directory('aclImdb/train', batch_size=batch_size, validation_split=0.2, subset='training', shuffle=True, seed=42) # 625 batches of 32 texts for training set randomly chosen\n",
        "raw_val_ds = tf.keras.utils.text_dataset_from_directory('aclImdb/train', batch_size=batch_size, validation_split=0.2, subset='validation', shuffle=True, seed=42) # 157 batches of 32 texts for validation set randomly chosen\n",
        "raw_test_ds = tf.keras.utils.text_dataset_from_directory('aclImdb/test', batch_size=batch_size) # 782 batches of 32 texts for test set\n",
        "\n",
        "first_batch_train = raw_train_ds.take(1) # get the first batch of the training text set"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t2iM2eQ0wyVk",
        "outputId": "0aa922df-694b-48c2-a50e-a6b648c210c5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Here's an extract from a negative review:\n",
            "b'\"Pandemonium\" is a horror movie spoof that comes off more stupid than funny. Believe me when I tell you, I love comedies. Especially comedy spoofs. \"Airplane\", \"The Naked Gun\" trilogy, \"Blazing Saddles\", \"High Anxiety\", and \"Spaceballs\" are some of my favorite comedies that spoof a particular genre. \"Pandemonium\" is not up there with those films. Most of the scenes in this movie had me sitting there in stunned silence because the movie wasn\\'t all that funny. There are a few laughs in the film, but when you watch a comedy, you expect to laugh a lot more than a few times and that\\'s all this film has going for it. Geez, \"Scream\" had more laughs than this film and that was more of a horror film. How bizarre is that?<br /><br />*1/2 (out of four)'\n"
          ]
        }
      ],
      "source": [
        "show_text(first_batch_train) # show an example of movie review"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CpSzgX8abQGE"
      },
      "source": [
        "---\n",
        "\n",
        "\n",
        "#### **2. Pre-processing & reformating data**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pIzLCu7V6ubd"
      },
      "source": [
        "###### **2.1. How to switch from a verbal language to a machine language?**\n",
        "\n",
        "The textual data is pre-processed and converted before being used by the model. Three crucial phases are established:\n",
        "- standardization\n",
        "- tokenization\n",
        "- vectorization\n",
        "\n",
        "The first pre-processing step standardizes text data by replacing upper case with lower case letters, by removing html tags and punctuation characters. The variable `punctuation` contains all punctuation characters to be removed. Special characters such as those with accents are not present in English texts. A standardization function is declared and used to process training data in the same way as other data (avoid training/testing bias)."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "ponctuation = re.escape(string.punctuation)\n",
        "print(\"The punctuation characters to be eliminated are: \\n\", \" \\n\", ponctuation)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "spQOTob2ZULo",
        "outputId": "6d53235f-3fbb-4910-fa0a-3a4e490cda8e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The punctuation characters to be eliminated are: \n",
            "  \n",
            " !\"\\#\\$%\\&'\\(\\)\\*\\+,\\-\\./:;<=>\\?@\\[\\\\\\]\\^_`\\{\\|\\}\\~\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dUnwN3b6x55J"
      },
      "outputs": [],
      "source": [
        "def standardization(input_data): # standardization function\n",
        "  no_uppercases = tf.strings.lower(input_data) # convert upper cases into lower cases...\n",
        "  no_html = tf.strings.regex_replace(no_uppercases, '<br />', ' ') # ... then remove HTML strings and...\n",
        "  no_punctuation = tf.strings.regex_replace(no_html, '[%s]' % ponctuation, '') # ... punctuation\n",
        "  return no_punctuation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IovmWK-uPcZA"
      },
      "source": [
        "The second pre-processing step is to transform strings of all texts into integers (=neural network inputs). Text examples are split into \"substrings\" (= word) and then recombined into ***tokens*** (tokenization step). For each text example, the token number is limited to the 10,000 most frequent words in the dataset. Any words beyond this limit will be ignored.\n",
        "\n",
        "These 10,000 tokens forme a dictionnary called ***vocabulary***. The vocabulary size is set to 10,000 in order to keep high-interest words (verbs, adjectives, common nouns) and get rid of rare words (names, etc.). A fixed length of the text sequences after vectorization is set to 1,000 tokens. If the text is shorter than 1000 tokens, it will be patch to reach this length. If it longer, it will be truncated.\n",
        "\n",
        "The Keras layer designed specifically for text preprocessing `TextVectorization` which transforms strings into sequences of integers that the neural network can process."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "w5_uvCYKx8Co"
      },
      "outputs": [],
      "source": [
        "max_features, sequence_length = 10000, 1000 # maximum number of words (=tokens) to consider in the vocabulary.\n",
        "vectorize_layer = tf.keras.layers.TextVectorization(standardize=standardization, max_tokens=max_features, output_mode='int', output_sequence_length=sequence_length) # transform strings into sequences of integers\n",
        "\n",
        "vectorize_layer.adapt(raw_train_ds.map(lambda x, y: x)) # vectorization layer to learn the vocabulary from the raw texts in the training dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###### **2.2. How do you visualize these transformations?**\n",
        "\n",
        "The first function `token_to_int` shows the transformation of text elements into integers. The second `int_to_token`is used to identify which token is associated with a given integer."
      ],
      "metadata": {
        "id": "a4HNDvV0LJTE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def token_to_int(raw_text):\n",
        "  raw_text = next(iter(raw_text))[0]\n",
        "  print(\"Review before tokenization and vectorization: \\n\", raw_text[0])\n",
        "  print(\" \")\n",
        "  text_vectorized = tf.expand_dims(raw_text, -1) #\n",
        "  text_vectorized = vectorize_layer(text_vectorized)\n",
        "  print(\"Review after tokenization and vectorization: \\n\", text_vectorized[0])\n",
        "\n",
        "token_to_int(raw_train_ds)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GlardaNcYWqD",
        "outputId": "e8699512-b7ff-4b9a-cac5-cd734cb52e71"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Review before tokenization and vectorization: \n",
            " tf.Tensor(b'Great movie - especially the music - Etta James - \"At Last\". This speaks volumes when you have finally found that special someone.', shape=(), dtype=string)\n",
            " \n",
            "Review after tokenization and vectorization: \n",
            " tf.Tensor(\n",
            "[  86   17  260    2  222    1  571   31  229   11 2418    1   51   22\n",
            "   25  404  251   12  306  282    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0], shape=(1000,), dtype=int64)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def int_to_token(index):\n",
        "  token = vectorize_layer.get_vocabulary()[index]\n",
        "  print(\"The integer %d represent the token: %s\" %(index, token))\n",
        "\n",
        "int_to_token(2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zvLRLjSODwsi",
        "outputId": "5e667bc6-b19b-4769-df46-97d846efc9c5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The integer 2 represent the token: the\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hDs5MOc7Usll"
      },
      "source": [
        "---\n",
        "\n",
        "\n",
        "#### **3. Model and training**\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###### **3.1. How to configure the datasets for performance?**\n",
        "\n",
        "During the model training phase, the duration represents the time required to open the data file, read it and train with it. By default, these steps are performed one at a time. With a prefetch method, the model opens the data file, then executes a training step `s` and loads the data for step `s+1` at the same time. Loading the batch in the background during the training phase enables more efficient use of available computing resources, avoiding the risk of a \"bottleneck\" where computation (GPU) is limited by the speed at which data is supplied (I/O).\n",
        "\n",
        "The overlapping of these steps is ensured by the `prefetch()` function, where the size of the prefetch buffer is automatically set by TensorFlow via the parameter `AUTOTUNE`.\n",
        "\n",
        "The function `cache()` temporarily stores transformed data of a batch (transformations detailed below) in the RAM memory (12.7 GB RAM for a Google configuration). This method saves a significant amount of time since complex transformations applied to the texts are only performed during the first epoch and re-used for the others."
      ],
      "metadata": {
        "id": "eVadUbUyOSpz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "AUTOTUNE = tf.data.AUTOTUNE # prefetch buffer size parameter\n",
        "\n",
        "raw_train_ds = raw_train_ds.cache().prefetch(buffer_size=AUTOTUNE) # store raw_train_ds temporarily in the RAM + load the next batch in background with a prefetch buffer size computed by AUTOTUNE\n",
        "raw_val_ds = raw_val_ds.cache().prefetch(buffer_size=AUTOTUNE)\n",
        "raw_test_ds = raw_test_ds.cache().prefetch(buffer_size=AUTOTUNE)"
      ],
      "metadata": {
        "id": "Y5W-pc4I8Wk3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###### **3.2. What is a neural network formed of?**"
      ],
      "metadata": {
        "id": "rQnGHKrYOfhQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def create_model():\n",
        "  embedding_dim = 16\n",
        "  model = tf.keras.Sequential()\n",
        "  model.add(vectorize_layer) # pre-processing layer (standardization + tokenization + vectorization)\n",
        "  model.add(tf.keras.layers.Embedding(max_features, embedding_dim))\n",
        "  model.add( tf.keras.layers.Dropout(0.2))\n",
        "  model.add(tf.keras.layers.GlobalAveragePooling1D())\n",
        "  model.add(tf.keras.layers.Dropout(0.2))\n",
        "  model.add(tf.keras.layers.Dense(1, activation='sigmoid')) # 1 output: propbability\n",
        "\n",
        "  return model"
      ],
      "metadata": {
        "id": "ZVy751wzln8H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###### **3.3. How to train the model?**\n"
      ],
      "metadata": {
        "id": "wnY6ZDN9OsPe"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "akEY74O-4mGl"
      },
      "outputs": [],
      "source": [
        "if training_phase == \"Yes\":\n",
        "  checkpoint_path = \"02_classification_text.weights.h5\"\n",
        "  cp_callback = tf.keras.callbacks.ModelCheckpoint(filepath=checkpoint_path, save_weights_only=True, verbose=0)\n",
        "  stop_early = tf.keras.callbacks.EarlyStopping(monitor='val_binary_accuracy', patience=10, restore_best_weights=True, min_delta=0.001)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model = create_model()\n",
        "model.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MZ-W38br25go",
        "outputId": "b50cc1a6-b7a0-41d3-e005-0c6772d0f854"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential_1\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " text_vectorization_1 (Text  (None, 1000)              0         \n",
            " Vectorization)                                                  \n",
            "                                                                 \n",
            " embedding_1 (Embedding)     (None, 1000, 16)          160000    \n",
            "                                                                 \n",
            " dropout_2 (Dropout)         (None, 1000, 16)          0         \n",
            "                                                                 \n",
            " global_average_pooling1d_1  (None, 16)                0         \n",
            "  (GlobalAveragePooling1D)                                       \n",
            "                                                                 \n",
            " dropout_3 (Dropout)         (None, 16)                0         \n",
            "                                                                 \n",
            " dense_1 (Dense)             (None, 1)                 17        \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 160017 (625.07 KB)\n",
            "Trainable params: 160017 (625.07 KB)\n",
            "Non-trainable params: 0 (0.00 Byte)\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "urPAjUL-ydCq",
        "outputId": "c6b12e8e-d86b-435e-d915-db9a01417908"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'TensorFlow'...\n",
            "remote: Enumerating objects: 71, done.\u001b[K\n",
            "remote: Counting objects: 100% (71/71), done.\u001b[K\n",
            "remote: Compressing objects: 100% (71/71), done.\u001b[K\n",
            "remote: Total 71 (delta 30), reused 0 (delta 0), pack-reused 0 (from 0)\u001b[K\n",
            "Receiving objects: 100% (71/71), 14.99 MiB | 14.88 MiB/s, done.\n",
            "Resolving deltas: 100% (30/30), done.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:Skipping variable loading for optimizer 'Adam', because it has 1 variables whereas the saved optimizer has 7 variables. \n"
          ]
        }
      ],
      "source": [
        "model.compile(loss=tf.keras.losses.BinaryCrossentropy(), optimizer='adam', metrics=[tf.metrics.BinaryAccuracy()])\n",
        "\n",
        "if training_phase == \"Yes\":\n",
        "  history = model.fit(raw_train_ds, validation_data=raw_val_ds, epochs=100, callbacks=[stop_early, cp_callback], verbose=0)\n",
        "  val_acc_per_epoch = history.history['val_binary_accuracy'] # best val_binary_accuracy achived at epoch 32\n",
        "  best_epoch = val_acc_per_epoch.index(max(val_acc_per_epoch)) + 1\n",
        "  print('Best epoch: %d' % (best_epoch))\n",
        "else:\n",
        "  !git clone https://github.com/Benjamin-morel/TensorFlow.git # go to the Github repertory TensorFlow and clone it\n",
        "  model.load_weights(\"TensorFlow/02_classification_text.weights.h5\") # import weights from the cloned repertory\n",
        "  !rm -rf TensorFlow/ # delete the cloned repertory"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iXkzAUtK6Kg4"
      },
      "outputs": [],
      "source": [
        "def show_evolution(history, val):\n",
        "  history_dict = history.history\n",
        "  history_dict.keys() # dictionnary: ['accuracy', 'loss', 'val_accuracy', 'val_loss']\n",
        "\n",
        "  if val == False: # get either the training set accuracy or the validation set accuracy\n",
        "    acc_train = history_dict['binary_accuracy']\n",
        "  else:\n",
        "    acc_train = history_dict['val_binary_accuracy']\n",
        "\n",
        "  epochs = range(1, len(acc_train) + 1)\n",
        "\n",
        "  fig = px.line(x = epochs, y = acc_train, width=600, height=400)\n",
        "  fig.update_layout(legend=dict(x=0.02, y=0.98, xanchor='left', yanchor='top', bgcolor='rgba(255, 255, 255, 0.8)', bordercolor='black', borderwidth=1))\n",
        "  if val == False: fig.update_traces(name=\"training\", showlegend=True)\n",
        "  else: fig.update_traces(name=\"validation\", showlegend=True)\n",
        "  fig.update_xaxes(title = \"epochs\"), fig.update_yaxes(title = \"binary_accuracy\")\n",
        "  fig.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "awd3_-nj6DQZ"
      },
      "outputs": [],
      "source": [
        "if training_phase == 'Yes':\n",
        "  show_evolution(history, True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6VZ-HWnoU19c"
      },
      "source": [
        "---\n",
        "\n",
        "\n",
        "#### **4. Evaluation and prediction**\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SgYqxc_pyhX-"
      },
      "outputs": [],
      "source": [
        "loss, accuracy = model.evaluate(raw_test_ds)\n",
        "\n",
        "print(f\"\"\"\n",
        "  {round(100*accuracy, 3)}% of the test set is corretly predicted\n",
        "  \"\"\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jPO0X7THurRd"
      },
      "source": [
        "It is now possible to use the trained model to recognize the sentiment of the author of a film review. Write your own review in the next section and check the model's prediction."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EqhYu-ySJa5a"
      },
      "outputs": [],
      "source": [
        "my_review = [\"This movie was terrible and boring. Most of scenes were violents and useless.\"]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h1pa8S_v6VEJ",
        "outputId": "b7bf9231-d8ae-4145-84df-1bc1743f20c0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1/1 [==============================] - 0s 25ms/step\n",
            "The movie looks pretty bad. (Model sures at  86.2 %)\n"
          ]
        }
      ],
      "source": [
        "examples = tf.constant(my_review)\n",
        "\n",
        "prediction = model.predict(examples)\n",
        "if prediction < 0.5:\n",
        "  print(\"The movie looks pretty bad. (Model sures at \", round(100*(1-prediction)[0][0], 1), \"%)\")\n",
        "else:\n",
        "  print(\"Great movie, go see it in the cinema! (Model sures at \", round(100*prediction[0][0], 1), \"%)\")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "V28",
      "authorship_tag": "ABX9TyMLzBFUEofAsk/eB8SKzoS1",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "TPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}