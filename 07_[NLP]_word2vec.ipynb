{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMuhu6tW7d8HhO+/dVQbnTh",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Benjamin-morel/TensorFlow/blob/main/07_%5BNLP%5D_word2vec.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "\n",
        "\n",
        "# **Machine Learning Model: word2vec and skip-gram**\n",
        "\n",
        "| | |\n",
        "|------|------|\n",
        "| Filename | 07_[NLP]_word2vec.ipynb |\n",
        "| Author(s) | Benjamin (contact.upside830@silomails.com) |\n",
        "| Date | January 26, 2024 |\n",
        "| Aim(s) | build a word embedding space with the skip-gram method |\n",
        "| Dataset(s) | IMDb Movie Reviews dataset [[1]](https://aclanthology.org/P11-1015.pdf)|\n",
        "| Version | Python 3.10.12 - TensorFlow 2.17.1 |\n",
        "\n",
        "\n",
        "<br> **!!Read before running!!** <br>\n",
        "* **Step 1.** Fill in the inputs.\n",
        "* **Step 2.** GPU execution is recommended for training. Yet, a pre-trained model can be used by running this notebook on a CPU. Using a pre-trained model saves time, computer resources and CO2 emissions.\n",
        "* **Step 3.** Run all and read comments.\n",
        "\n",
        "---\n",
        "\n",
        "#### **Motivation**\n",
        "\n",
        "With the IMDb dataset used in code [02_classfication_text.ipynb](https://github.com/Benjamin-morel/TensorFlow/tree/main), the model is trained using a word2vec technique and model's weights are then used to construct a word embedding space.  \n",
        "\n",
        "#### **Outline**\n",
        "\n",
        "*   Input section\n",
        "*   Python librairies & display utilities\n",
        "*   Data retrieval & set generation\n",
        "*   Embedding word space construction\n",
        "*   Classification model & training\n",
        "*   Embedding space exploration\n",
        "*   References\n",
        "\n",
        "---"
      ],
      "metadata": {
        "id": "IKO5J-_-Vj3U"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **0. Input section**"
      ],
      "metadata": {
        "id": "an7V4rZe30MH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "text_nb = 25000\n",
        "max_features = 5000\n",
        "max_length = 20"
      ],
      "metadata": {
        "id": "JvszvjSG37h-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "---\n",
        "\n",
        "## **1. Python librairies & display utilities**"
      ],
      "metadata": {
        "id": "iOr8jVUqvebl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# @title 1.1. Python librairies [RUN ME]\n",
        "\n",
        "\"\"\"math\"\"\"\n",
        "import numpy as np # linear algebra\n",
        "import sklearn.metrics # scores and evaluation metrics\n",
        "\n",
        "\"\"\"file opening and pre-processing\"\"\"\n",
        "import os # miscellaneous operating system interfaces\n",
        "import pandas as pd # data manipulation tool\n",
        "from re import escape # regular expressions\n",
        "import string # string manipulation\n",
        "import shutil # operations on files\n",
        "\n",
        "\"\"\"ML models\"\"\"\n",
        "import tensorflow as tf # framework for ML/DL\n",
        "from tensorflow import keras # API used to build model in TensorFlow\n",
        "\n",
        "\"\"\"display and export\"\"\"\n",
        "import matplotlib.pyplot as plt # graphing package\n",
        "import pickle # serialization\n",
        "\n",
        "\"\"\"performances\"\"\"\n",
        "from time import time # timer\n",
        "start = time()\n",
        "device = tf.config.list_physical_devices(device_type=None)[-1][-1]"
      ],
      "metadata": {
        "cellView": "form",
        "id": "Rc2oQxKkvr-C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title 1.2. Import Github files [RUN ME]\n",
        "\n",
        "\"\"\"clone the Github repertory TensorFlow and imports the files required (see section 3.2)\"\"\"\n",
        "def get_github_files():\n",
        "  !git clone https://github.com/Benjamin-morel/TensorFlow.git TensorFlow_duplicata\n",
        "  path_model = 'TensorFlow_duplicata/99_pre_trained_models/07_word2vec/07_word2vec.keras'\n",
        "  path_classifier = 'TensorFlow_duplicata/99_pre_trained_models/07_word2vec/07_classification_text.keras'\n",
        "  path_dictionary = 'TensorFlow_duplicata/99_pre_trained_models/07_word2vec/dictionary_embedding.pkl'\n",
        "  path_vocab = 'TensorFlow_duplicata/99_pre_trained_models/07_word2vec/vocab.pkl'\n",
        "  model = keras.models.load_model(path_model, custom_objects={'Word2Vec': Word2Vec})\n",
        "  classifier = keras.models.load_model(path_classifier)\n",
        "  with open(path_dictionary, 'rb') as f:\n",
        "    dictionary = pickle.load(f)\n",
        "  with open(path_vocab, 'rb') as f:\n",
        "    vocab = pickle.load(f)\n",
        "  !rm -rf TensorFlow_duplicata/\n",
        "  return model, dictionary, vocab, classifier"
      ],
      "metadata": {
        "cellView": "form",
        "id": "hJrHq8awwNgH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title 1.3. Figure plots [RUN ME]\n",
        "\n",
        "\"\"\"plot confusion matrix and ROC curve\"\"\"\n",
        "def plot_evaluation(test_dataset, predictions, labels=[0,1]):\n",
        "  actuals = tf.concat([y for x, y in test_dataset], axis=0)\n",
        "  predicted_labels = np.round(predictions, 0)\n",
        "\n",
        "  plt.figure(figsize=(12,5))\n",
        "\n",
        "  confusion_mat = tf.math.confusion_matrix(actuals,predicted_labels) # columns = prediction labels / rows = real labels\n",
        "\n",
        "  # Subplot 1: Confusion matrix\n",
        "  plt.subplot(1,2,1, xlabel=\"Predictions\", ylabel=\"Actuals\", title=\"Confusion matrix of the test set\", xticks=[0,1], yticks=[0,1])\n",
        "  plt.imshow(confusion_mat)\n",
        "\n",
        "  for i in range(confusion_mat.shape[0]): # annotation in each cnfusion matrix cell\n",
        "      for j in range(confusion_mat.shape[1]):\n",
        "          plt.text(x=j, y=i,s=int(confusion_mat[i, j]), va='center', ha='center', size='large')\n",
        "\n",
        "  # Subplot 2: ROC curve\n",
        "  fpr, tpr, _ = sklearn.metrics.roc_curve(actuals,  predictions.ravel())\n",
        "  plt.subplot(1,2,2, xlabel=\"FPR\", ylabel=\"TPR\", title=\"ROC curve\")\n",
        "  plt.plot(fpr, tpr)\n",
        "\n",
        "  # Metrics info\n",
        "  accuracy = sklearn.metrics.accuracy_score(actuals, predicted_labels)\n",
        "  recall = sklearn.metrics.recall_score(actuals, predicted_labels)\n",
        "  F1_score = sklearn.metrics.f1_score(actuals, predicted_labels)\n",
        "\n",
        "  print(\"############################\")\n",
        "  print(\"Evaluation on the test set: \")\n",
        "  print(\"############################\")\n",
        "  print(\"Accuracy...{:.4f}\" .format(accuracy))\n",
        "  print(\"Recall.....{:.4f}\" .format(recall))\n",
        "  print(\"F1-score...{:.4f}\" .format(F1_score))\n",
        "\n",
        "  plt.show()"
      ],
      "metadata": {
        "cellView": "form",
        "id": "CH0tm36GQlQ3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "\n",
        "\n",
        "## **2. Data retrieval and set generation**"
      ],
      "metadata": {
        "id": "vTJzD6Tv4Fca"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The skip-gram method is a word embedding technique used in Word2Vec to learn vector representations of words based on their surrounding context. It works by selecting a target word and predicting its context words within a given window size.\n",
        "\n",
        ">Example: \"This horror movie was so terrifying that I screamed!\"\n",
        "\n",
        ">Window = 1<br>\n",
        ">Target word: \"terrifying\": \"This horror movie was [**so terrifying that**] I screamed!\" <br>\n",
        ">Context words: \"so\" and \"that\"\n",
        "\n",
        "With a larger window to capture more context:\n",
        "\n",
        ">Window = 3 <br>\n",
        ">Target word: \"terrifying\": \"This horror [**movie was so terrifying that I screamed**]!\" <br>\n",
        ">Context words: \"movie\", \"was\", \"so\", \"that\", \"I\" and \"screamed\"\n",
        "\n",
        "Each word is converted to an integer and passed through a neural network with a single hidden layer. The model is trained to maximize the probability of context words given the target word, often using negative sampling to improve efficiency.\n",
        "\n",
        "After training, the hidden layer weights form a word embeddings space, where words are represented by a 16-dimensional embedding vector. Skip-gram is especially useful for capturing relationships between words.\n"
      ],
      "metadata": {
        "id": "DvEtr1KoFGLk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2.1. Retrieve data"
      ],
      "metadata": {
        "id": "5k2EbRgE203B"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The IMDb Movie Reviews is a database created for sentiment analysis in movie reviews. It contains 50,000 movie reviews. The database is extracted and placed in the folder review_dataset. The architecture of review_dataset is as follows:\n",
        "\n",
        "```markdown\n",
        "**Imdb_dataset/**\n",
        ". . . aclImdb/\n",
        ". . . . . . train/\n",
        ". . . . . . test/\n",
        ". . . . . . README\n",
        ". . . . . . imdb.vocab\n",
        ". . . . . . imdbEr.text\n",
        "```\n",
        "\n",
        "Other files are included in the folder like `README` which provides information about the dataset and how to use it. Files `imdb.vocab` and `imdbEr.txt` contain additional information about errors, URL website and specific annotations.\n",
        "\n",
        "To train the model, the labels used to classify are not required."
      ],
      "metadata": {
        "id": "CCedTxGVU898"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_data(url, batch_size):\n",
        "\n",
        "  dataset_name = \"Imdb_dataset_1\"\n",
        "  path = tf.keras.utils.get_file(dataset_name, url, extract=True)\n",
        "  path = os.path.join(path, 'aclImdb')\n",
        "  train_path = os.path.join(path, 'train')\n",
        "  test_path = os.path.join(path, 'test')\n",
        "  remove_dir = os.path.join(train_path, 'unsup') # remove the folder with unlabeled reviews for unsupervised learning\n",
        "  shutil.rmtree(remove_dir)\n",
        "\n",
        "  # data used to build embedding word space with the skip-gram method\n",
        "  data_skip_gram = tf.keras.utils.text_dataset_from_directory(train_path, labels=None, batch_size=batch_size, validation_split=None, shuffle=True, seed=1, verbose=0)\n",
        "\n",
        "  # data for classification model (same as 02_classification_text.ipynb)\n",
        "  raw_train_ds = keras.utils.text_dataset_from_directory(train_path, batch_size=32, validation_split=0.2, subset='training', shuffle=True, seed=1)\n",
        "  raw_val_ds = keras.utils.text_dataset_from_directory(train_path, batch_size=32, validation_split=0.2, subset='validation', shuffle=True, seed=1)\n",
        "  raw_test_ds = keras.utils.text_dataset_from_directory(test_path, batch_size=32)\n",
        "\n",
        "  return data_skip_gram, raw_train_ds, raw_val_ds, raw_test_ds"
      ],
      "metadata": {
        "id": "6tyDasAh_ZyY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "url = \"https://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz\"\n",
        "\n",
        "batch_size = 1\n",
        "\n",
        "data_skip_gram, raw_train_ds, raw_val_ds, raw_test_ds = get_data(url, batch_size)"
      ],
      "metadata": {
        "id": "sVSYard4_kii",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d89053cd-1681-41b3-ae1d-aaa531d9b253"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz\n",
            "\u001b[1m84125825/84125825\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m49s\u001b[0m 1us/step\n",
            "Found 25000 files belonging to 2 classes.\n",
            "Using 20000 files for training.\n",
            "Found 25000 files belonging to 2 classes.\n",
            "Using 5000 files for validation.\n",
            "Found 25000 files belonging to 2 classes.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The batch size is 1 to facilitate pre-processing of texts. The variable `nb_text` is used to control the number of texts to be supplied to the model.\n",
        "\n"
      ],
      "metadata": {
        "id": "d5rIxD1iGh-q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data_skip_gram = data_skip_gram.take(text_nb)\n",
        "\n",
        "for text in data_skip_gram.take(1):\n",
        "  print(\"Here, an example of text data used for the skip-gram method: \\n\", text.numpy())"
      ],
      "metadata": {
        "id": "K9xSdAZOT2JD",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c3e380ac-0e8b-410c-ef1c-023bcb98263c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Here, an example of text data used for the skip-gram method: \n",
            " [b'\"Dressed to Kill\" is Brian DePalma\\'s best film, an absolute thrill ride of suspense, humor and style that remains unrivaled today. DePalma has a bum rap in Hollywood, as most people claim that he rips off Alfred Hitchcock. He does not. Hitchcock could only dream of what DePalma shows in his thrillers.<br /><br />Sadly, the original uncut version of \"Dressed to Kill\" is no longer available on video. The current copy released by Goodtimes is the Jack Valenti approved R rated cut. But some copies of DePalma\\'s original cut still exist. It is the one distributed by Warner Home Video, in both a green cardboard box with Angie Dickinson on the cover, or in a black clamshell case with the theatrical poster on silver lining. These are the ones to get, if you can find a copy. I have the green one and it is among my treasured possessions.<br /><br />Anyway, back to the story. Dickinson plays Kate Miller, a sexually frustrated wife who is being treated by Dr. Robert Elliott ( Michael Caine) for her obsessive fantasies. While on a trip to the museum (a real tour-de-force for DePalma in terms of camera work and suspense), <br /><br />Miller is picked up by a stranger. You can pretty much guess what happens to her, since the ads and box art give away the story. But there are a few complications. A hooker (Nancy Allen) is the sole witness to the murder. Kate Miller\\'s son Peter (Keith Gordon) is a teenage genius determined to solve the crime. And Dr. Elliott\\'s answering machine has a certain message on it...about a missing razor...<br /><br />I\\'m not spoiling the film at all for you since what I have described above takes place within the opening half hour and DePalma\\'s biggest surprises are reserved for the last hour. This film is explicit, however, enough for Valenti (head of the MPAA) to demand cuts in the film. What surprises me is what cuts he wanted. Several cuts in the opening shower scene and one or two slashing scenes and some of Nancy Allen\\'s dialogue (Valenti wanted \"cock\" changed to \"bulge\"). This is a film that is very violent and bloody, yet the objections are to sexual content. I\\'d love to hear Valenti\\'s explaination at how seeing two women in a tender love scene in \"Lost and Delirious\" is somehow more damaging to a young mind than Arnold Schwarzenegger blowing away people with a chain gun. It just isn\\'t fair.<br /><br />What makes \"Dressed to Kill\" so good is that not only are the technical credits first rate, but the performances are very good as well. Michael Caine, who was in a lot of crap in this time period, gives one of his best performances as the doctor. Angie Dickinson is better than usual, possibly because she actually has a strong role here. Nancy Allen adds this to her range of performances that has her pegged as one of the most underrated and overlooked actresses in the world. Keith Gordon is wonderful as the genius and i loved all those inventions of his.<br /><br />DePalma is one of our best directors who has never received the recognition he deserved. The recent joke of the AFI 100 Best Thrillers list showed that very few people actually know what a thriller is. If they were to actually open their eyes for once, they would see that DePalma has staked his career in thrillers and is actually the best craftsman. This is even better than \"Psycho\". It\\'s a shame that very few actually know that.<br /><br />**** out of 4 stars']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2.2. Pre-processing"
      ],
      "metadata": {
        "id": "bmEWej3C24FC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Input data must be processed:\n",
        "```markdown\n",
        "Tensor(['The film was ....'])\n",
        "Tensor(['I see this movie ...'])\n",
        ". . .\n",
        "```\n",
        "Each tensor is composed of a sentence (and not an entire movie review). So, for one review, several tensors can be generated. It is assumed that each sentence carries a context/meaning. A sentence is defined here as a series of words ending by a `. `, a `? ` or a ' ! `."
      ],
      "metadata": {
        "id": "t7dr6A2lWIJS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "if device == \"GPU\":\n",
        "  punctuation_to_remove = escape(string.punctuation).replace('.', '').replace('?', '').replace('!', '')"
      ],
      "metadata": {
        "id": "2HK0RxBRxaKr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The 50 most frequent words in the dataset are also removed. This list of words comes from the Jupyter Notebook [02_classfication_text.ipynb](https://github.com/Benjamin-morel/TensorFlow/tree/main). These words are removed because they carry a little meaning and increase the size of the model's input vectors:\n",
        "\n",
        "```markdown\n",
        "The dog barked loudly --> dog barked loudly --> ['dog', 'barked', 'loudly']\n",
        "```"
      ],
      "metadata": {
        "id": "Hcbf6V3WQNkC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "if device == \"GPU\":\n",
        "  token_common = [' the ', ' and ', ' a ', ' of ', ' to ', ' is ', ' in ', ' it ', ' i ', ' this ', ' that ', ' br ', ' was ', ' as ', ' with ', ' for ',\n",
        "                  ' you ', ' on ', ' are ', ' one ', ' be ', ' he ', ' its ', ' have ', ' an ', ' by ', ' at ', ' all ', ' from ', ' who ', ' so ',\n",
        "                  ' they ', ' her ', ' just ', ' some ', ' out ', ' about ', ' or ', ' s ', ' if ', '  c  ', ' there ', ' were ', ' would ', ' had ', ' it ', ' we ']"
      ],
      "metadata": {
        "id": "ZCDiZgqznO3T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def custom_standardization_for_skip_gram(input_text):\n",
        "  text_modified = tf.strings.lower(input_text) # upper cases --> lower cases\n",
        "  text_modified = tf.strings.regex_replace(text_modified, '<br />', ' ') # remove HTML strings\n",
        "  for i in range(len(token_common)):\n",
        "    text_modified = tf.strings.regex_replace(text_modified, token_common[i], ' ') # remove frequent words\n",
        "  text_modified = tf.strings.regex_replace(text_modified, '[%s]' % punctuation_to_remove, '') # remove punctuation\n",
        "  return text_modified"
      ],
      "metadata": {
        "id": "WYbUfk3h31Ow"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if device == \"GPU\":\n",
        "  data_skip_gram = data_skip_gram.map(lambda x: custom_standardization_for_skip_gram(x))\n",
        "  for text in data_skip_gram.take(1):\n",
        "    print(\"Here is the result of the transformation: \\n\", text.numpy())"
      ],
      "metadata": {
        "id": "wS7XzSSMxj15",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5686d683-a624-434a-9ab9-a14bcf6f72d4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Here is the result of the transformation: \n",
            " [b'after watching film left two very annoyances film why did make chens character mcguyver hitman lees character such incompetent idiot? chens characters background raised underground cambodian orphanage blood thirsty fighter where learn brawl death like wild dogs. detail pushed early during scene where gets into cab starts drive shows how unfamiliar seat belt. soon after scene has similar situation dim sum restaurant. not only uneducated starving. not reference chens scrawny physique but two early scenes film where scarfing down food which being rice porridge off floor lower deck old ship. si first ten minutes film established chen malnutritioned unmodernizedand has only thing going him his dog brawling fighting style sort. despite situation chen manages outshoot every policeman even managing ricochet bullet off metal pipe hit guy head whom holding chens girlfriend hostage has somehow attained super human strength swings 50 lb block concrete plastered end metal pipe head police chief getting shot chest said chief.  now lees character...okay get it hes depressed hes got baggage but wow can do anything right? moment try make him cool composed ready take care business next moment got beat again. first scene runs into chen manages misses him approx 15 ft multiple times. toward end scene lee watches chen his close friend coworker gets slowly stabbed neck long knife good full 5 seconds while holding gun chen face 10 ft distance. even end movie lee manages get stabbed death fails once again.  my biggest problem movie presented manner film makers trying get audience sympathize chens character killing survive. lot easier didnt watch chen kill innocent people throughout whole awful movie. numerous people killed only two people intention trying kill him police chief lee. others people eating boat owners taxi drivers policemen trying arrest him not kill. overall chens character cold blooded killer kills what wants even free ride. did mention carrying wad hundred dollar bills throughout most film? my 3 stars go interesting directorcamera work got nice shots.  bottomline made nuthugging chen fans. me dog bite dvd']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "An example of a pre-processed movie review is displayed. Only the punctuation elements `.`, `?` and `!` are retained."
      ],
      "metadata": {
        "id": "87oEZsURRan_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "From these pre-processed texts, the sentences are separated and isolated into tensors."
      ],
      "metadata": {
        "id": "rQ3DJ4c6SJfA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def split_sentences(text):\n",
        "  text =  tf.strings.split(text, '. ')\n",
        "  text =  tf.strings.split(text, '? ')\n",
        "  text =  tf.strings.split(text, '! ')\n",
        "  return text"
      ],
      "metadata": {
        "id": "d-xpMZykxvWQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if device == \"GPU\":\n",
        "  data_skip_gram = data_skip_gram.map(lambda x: split_sentences(x))"
      ],
      "metadata": {
        "id": "pkgs2qz-xx-2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Finally, the tensors contained in `train_ds` - with variable size - are converted into individual scalar tensors."
      ],
      "metadata": {
        "id": "giPPN4gmULcU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def split_ragged_tensor(ragged_tensor):\n",
        "  flat_values = ragged_tensor.flat_values\n",
        "  return tf.data.Dataset.from_tensor_slices(flat_values)"
      ],
      "metadata": {
        "id": "4OqamkAlyKxb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if device == \"GPU\":\n",
        "  data_skip_gram = data_skip_gram.flat_map(split_ragged_tensor)\n",
        "  for text in data_skip_gram.take(5):\n",
        "    print(\"The skip-gram data are finally a series of sentences: \\n\", text.numpy())"
      ],
      "metadata": {
        "id": "sGE9NNatyMbM",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a59b00f2-4b5c-4945-a0cb-e1e3d82612d9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The skip-gram data are finally a series of sentences: \n",
            " b'i dont words describe how good movie is'\n",
            "The skip-gram data are finally a series of sentences: \n",
            " b'only genius like amrita pritam could written such real depiction days partition'\n",
            "The skip-gram data are finally a series of sentences: \n",
            " b'movie kept haunting me many days'\n",
            "The skip-gram data are finally a series of sentences: \n",
            " b' urmila did role life movie'\n",
            "The skip-gram data are finally a series of sentences: \n",
            " b'she put life role puroo manoj vajpai did no less his role rashid'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2.3. Vectorization"
      ],
      "metadata": {
        "id": "m5yFx-K027PG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "A maximum of n tokens (default: 20) are generated for each sentence and then vectorized."
      ],
      "metadata": {
        "id": "gO0xPfviSlL2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "vectorize_layer = tf.keras.layers.TextVectorization(standardize=None, # already done\n",
        "                                                    max_tokens=max_features,\n",
        "                                                    output_mode='int',\n",
        "                                                    output_sequence_length=max_length)"
      ],
      "metadata": {
        "id": "JKsOEFqVWFBL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if device == \"GPU\":\n",
        "  vectorize_layer.adapt(data_skip_gram)\n",
        "  data_skip_gram = data_skip_gram.map(vectorize_layer)"
      ],
      "metadata": {
        "id": "0mNNDtBL32_P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if device == \"GPU\":\n",
        "  word_list = vectorize_layer.get_vocabulary()\n",
        "  int_list = list(data_skip_gram.as_numpy_iterator())\n",
        "\n",
        "  for seq in int_list[:5]:\n",
        "    print(f\"{seq} => {[word_list[i] for i in seq]}\")"
      ],
      "metadata": {
        "collapsed": true,
        "id": "mVYcxH6OznQJ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "13e7537b-20f6-4fa2-bb92-871e6ff0bb32"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[ 79 250   2 297  22 469  31  44  22   0   0   0   0   0   0   0   0   0\n",
            "   0   0] => ['i', 'watched', 'movie', 'second', 'time', 'enjoyed', 'much', 'first', 'time', '', '', '', '', '', '', '', '', '', '', '']\n",
            "[  14  880  266    2   11   72   43  182 1191    0    0    0    0    0\n",
            "    0    0    0    0    0    0] => ['very', 'emotional', 'beautiful', 'movie', 'good', 'acting', 'great', 'family', 'values', '', '', '', '', '', '', '', '', '', '', '']\n",
            "[3566    1    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0] => ['inspiring', '[UNK]', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '']\n",
            "[1805  478 1582    1   75    4 1493 4063 1351  842  458  402 1230    1\n",
            "  636    0    0    0    0    0] => ['dressed', 'kill', 'brian', '[UNK]', 'best', 'film', 'absolute', 'thrill', 'ride', 'suspense', 'humor', 'style', 'remains', '[UNK]', 'today', '', '', '', '', '']\n",
            "[   1    9    1 3747  329   46   39 2221    1   88 4974 2942    0    0\n",
            "    0    0    0    0    0    0] => ['[UNK]', 'has', '[UNK]', 'rap', 'hollywood', 'most', 'people', 'claim', '[UNK]', 'off', 'alfred', 'hitchcock', '', '', '', '', '', '', '', '']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "\n",
        "\n",
        "## **3. Embedding word space construction**"
      ],
      "metadata": {
        "id": "shZUw7pEXayn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3.1. Skip-gram pair generation"
      ],
      "metadata": {
        "id": "HPnGyRw02_lC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Generates skip-gram pairs with negative sampling for a list of sequences (int-encoded sentences) based on window size, number of negative samples and vocabulary size."
      ],
      "metadata": {
        "id": "kbDFjB9AX9nR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_training_data(sequences, window_size, num_ns, vocab_size, seed):\n",
        "  targets, contexts, labels = [], [], []\n",
        "\n",
        "  sampling_table = tf.keras.preprocessing.sequence.make_sampling_table(vocab_size) # Build the sampling table for `vocab_size` tokens.\n",
        "\n",
        "  for sequence in sequences: # Iterate over all sequences (sentences) in the dataset\n",
        "\n",
        "    positive_skip_grams, _ = tf.keras.preprocessing.sequence.skipgrams(sequence,\n",
        "                                                                       vocabulary_size=vocab_size,\n",
        "                                                                       sampling_table=sampling_table,\n",
        "                                                                       window_size=window_size,\n",
        "                                                                       negative_samples=0) # Generate positive skip-gram pairs for a sequence (sentence)\n",
        "\n",
        "    for target_word, context_word in positive_skip_grams: # Iterate over each positive skip-gram pair to produce training examples with a positive context word and negative samples.\n",
        "\n",
        "      context_class = tf.expand_dims(tf.constant([context_word], dtype=\"int64\"), 1)\n",
        "\n",
        "      negative_sampling_candidates, _, _ = tf.random.log_uniform_candidate_sampler(true_classes=context_class,\n",
        "                                                                                   num_true=1,\n",
        "                                                                                   num_sampled=num_ns,\n",
        "                                                                                   unique=True,\n",
        "                                                                                   range_max=vocab_size,\n",
        "                                                                                   seed=seed,\n",
        "                                                                                   name=\"negative_sampling\")\n",
        "\n",
        "\n",
        "      context = tf.concat([tf.squeeze(context_class,1), negative_sampling_candidates], 0) # Build context and label vectors (for one target word)\n",
        "      label = tf.constant([1] + [0]*num_ns, dtype=\"int64\")\n",
        "\n",
        "      targets.append(target_word) # Append each element from the training example to global lists.\n",
        "      contexts.append(context)\n",
        "      labels.append(label)\n",
        "\n",
        "  return targets, contexts, labels"
      ],
      "metadata": {
        "id": "QA4gyGLV3qak"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if device == \"GPU\":\n",
        "  targets, contexts, labels = generate_training_data(sequences=int_list,\n",
        "                                                    window_size=2,\n",
        "                                                    num_ns=4,\n",
        "                                                    vocab_size=max_features,\n",
        "                                                    seed=1)\n",
        "\n",
        "  targets = np.array(targets)\n",
        "  contexts = np.array(contexts)\n",
        "  labels = np.array(labels)\n",
        "\n",
        "  print(f\"targets.shape: {targets.shape}\")\n",
        "  print(f\"contexts.shape: {contexts.shape}\")\n",
        "  print(f\"labels.shape: {labels.shape}\")"
      ],
      "metadata": {
        "id": "T_fLkogA3-gF",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2cab1819-fd31-45d6-8ff3-ae402ea413ea"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "targets.shape: (1808313,)\n",
            "contexts.shape: (1808313, 5)\n",
            "labels.shape: (1808313, 5)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3.2. Performances and batches"
      ],
      "metadata": {
        "id": "UgY7QoLBYBrf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "if device == \"GPU\":\n",
        "  batch_size = 1024\n",
        "  AUTOTUNE = tf.data.AUTOTUNE\n",
        "  dataset = tf.data.Dataset.from_tensor_slices(((targets, contexts), labels))\n",
        "  dataset = dataset.shuffle(len(targets)).batch(batch_size).repeat()\n",
        "\n",
        "  dataset = dataset.cache().prefetch(buffer_size=AUTOTUNE)"
      ],
      "metadata": {
        "id": "8NoVgzD-KC-S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3.3. Model definition"
      ],
      "metadata": {
        "id": "tiVadQe9YHMm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "@keras.utils.register_keras_serializable()\n",
        "class Word2Vec(tf.keras.Model):\n",
        "  def __init__(self, vocab_size, embedding_dim, **kwargs):\n",
        "    super(Word2Vec, self).__init__()\n",
        "    self.vocab_size = vocab_size # Store vocab_size as an attribute\n",
        "    self.embedding_dim = embedding_dim # Store embedding_dim as an attribute\n",
        "    self.target_embedding = tf.keras.layers.Embedding(vocab_size,\n",
        "                                      embedding_dim,\n",
        "                                      name=\"w2v_embedding\")\n",
        "    self.context_embedding = tf.keras.layers.Embedding(vocab_size,\n",
        "                                       embedding_dim)\n",
        "\n",
        "  def call(self, pair):\n",
        "    target, context = pair\n",
        "    # target: (batch, dummy?)  # The dummy axis doesn't exist in TF2.7+\n",
        "    # context: (batch, context)\n",
        "    if len(target.shape) == 2:\n",
        "      target = tf.squeeze(target, axis=1)\n",
        "    # target: (batch,)\n",
        "    word_emb = self.target_embedding(target)\n",
        "    # word_emb: (batch, embed)\n",
        "    context_emb = self.context_embedding(context)\n",
        "    # context_emb: (batch, context, embed)\n",
        "    dots = tf.einsum('be,bce->bc', word_emb, context_emb)\n",
        "    # dots: (batch, context)\n",
        "    return dots\n",
        "\n",
        "  def get_config(self): # Define get_config to include vocab_size and embedding_dim\n",
        "    config = super(Word2Vec, self).get_config()\n",
        "    config.update({\"vocab_size\": self.vocab_size, \"embedding_dim\": self.embedding_dim})\n",
        "    return config\n",
        "\n",
        "  @classmethod\n",
        "  def from_config(cls, config): # Define from_config to use vocab_size and embedding_dim\n",
        "      return cls(**config)"
      ],
      "metadata": {
        "id": "-Fmo3B7x4Ebi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def create_model(embedding_dim):\n",
        "  model = Word2Vec(max_features, embedding_dim)\n",
        "  model.compile(optimizer='adam',\n",
        "                loss=tf.keras.losses.CategoricalCrossentropy(from_logits=True),\n",
        "                metrics=None)\n",
        "  return model"
      ],
      "metadata": {
        "id": "_AUxMqyq9jq3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3.4. Training phase"
      ],
      "metadata": {
        "id": "_-D7FGkZYKg0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def train_model(model, training_data, steps_per_epoch, **kwargs):\n",
        "  kwargs.setdefault(\"epochs\", 3)\n",
        "  kwargs.setdefault(\"verbose\", 2)\n",
        "  log = model.fit(training_data, steps_per_epoch = steps_per_epoch, **kwargs)\n",
        "\n",
        "  return log.history[\"loss\"]"
      ],
      "metadata": {
        "id": "MmaQTLUD8bE0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if device == \"GPU\":\n",
        "  steps_per_epoch = len(targets) // batch_size\n",
        "  history = train_model(model, dataset, steps_per_epoch)\n",
        "  model.save('07_word2vec.keras')\n",
        "else:\n",
        "  model, dictionary_embedding, vocab, classifier_model = get_github_files()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WxdU_xWo-Fpk",
        "outputId": "c7d8faf0-5475-4228-820a-b0ee4b3e96dd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/3\n",
            "1765/1765 - 21s - 12ms/step - loss: 1.1974\n",
            "Epoch 2/3\n",
            "1765/1765 - 21s - 12ms/step - loss: 1.1973\n",
            "Epoch 3/3\n",
            "1765/1765 - 21s - 12ms/step - loss: 1.1971\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "\n",
        "\n",
        "## **4. Classification model and training**"
      ],
      "metadata": {
        "id": "jQVDTF20a9vD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 4.1. Pre-processing layer for classification model"
      ],
      "metadata": {
        "id": "VXKYegYQbEmU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "weights = model.get_layer('w2v_embedding').get_weights()[0]"
      ],
      "metadata": {
        "id": "EsQnN_yhbLEs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "@keras.utils.register_keras_serializable()\n",
        "def custom_standardization_classification(input_text):\n",
        "  no_uppercase = tf.strings.lower(input_text) # upper case --> lower case letters\n",
        "  no_html_uppercase = tf.strings.regex_replace(no_uppercase, '<br />', ' ') # remove HTML strings\n",
        "  no_punctuation_html_uppercase = tf.strings.regex_replace(no_html_uppercase, '[%s]' % escape(string.punctuation), '') # remove punctuation\n",
        "  return no_punctuation_html_uppercase"
      ],
      "metadata": {
        "id": "L9ysUj4JbPOb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "raw_train_ds = raw_train_ds.map(lambda x, y: (custom_standardization_classification(x), y))\n",
        "raw_test_ds = raw_test_ds.map(lambda x, y: (custom_standardization_classification(x), y))\n",
        "raw_val_ds = raw_val_ds.map(lambda x, y: (custom_standardization_classification(x), y))"
      ],
      "metadata": {
        "id": "3tGceeJhbTyd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "max_length = 500 # truncate to the 500th word\n",
        "\n",
        "vectorize_layer1 = keras.layers.TextVectorization(standardize=None,\n",
        "                                                 max_tokens=max_features,\n",
        "                                                 output_sequence_length=max_length,\n",
        "                                                 output_mode='int',\n",
        "                                                 vocabulary = word_list\n",
        "                                                 )"
      ],
      "metadata": {
        "id": "gyUkUs0AbXdC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "AUTOTUNE = tf.data.AUTOTUNE\n",
        "\n",
        "raw_train_ds = raw_train_ds.cache().prefetch(buffer_size=AUTOTUNE)\n",
        "raw_test_ds = raw_test_ds.cache().prefetch(buffer_size=AUTOTUNE)\n",
        "raw_val_ds = raw_val_ds.cache().prefetch(buffer_size=AUTOTUNE)"
      ],
      "metadata": {
        "id": "7e6_fEX5bdv5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 4.2. Classification model"
      ],
      "metadata": {
        "id": "MpvsmZXcbf-X"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "custom_embedding_layer = keras.Sequential([keras.layers.Embedding(input_dim = max_features, output_dim = 16, embeddings_initializer=keras.initializers.Constant(weights))])"
      ],
      "metadata": {
        "id": "8sPG9_jCbj49"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "custom_embedding_layer.trainable = False"
      ],
      "metadata": {
        "id": "Pm68bjsObl6v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def create_classifier():\n",
        "  embedding_dim = 16\n",
        "  model = keras.Sequential([vectorize_layer1, # pre-processing layer\n",
        "                            custom_embedding_layer,\n",
        "                            keras.layers.Dropout(0.2),\n",
        "                            keras.layers.Dense(16, activation='relu'),\n",
        "                            keras.layers.GlobalAveragePooling1D(),\n",
        "                            keras.layers.Dropout(0.2),\n",
        "                            keras.layers.Dense(1, activation='sigmoid')\n",
        "                            ], name=\"classification_text_model\")\n",
        "\n",
        "  model.compile(loss=keras.losses.BinaryCrossentropy(from_logits=False),\n",
        "                optimizer=keras.optimizers.Adam(learning_rate=0.001),\n",
        "                metrics=[tf.metrics.BinaryAccuracy(name=\"binary_accuracy\", threshold=0.5)])\n",
        "\n",
        "  return model"
      ],
      "metadata": {
        "id": "o91K6dGRbttb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train_classifier(model, training_data, validation_data, callbacks, **kwargs):\n",
        "  kwargs.setdefault(\"epochs\", 5)\n",
        "  kwargs.setdefault(\"verbose\", 1)\n",
        "  log = model.fit(training_data, validation_data=validation_data, callbacks=callbacks, **kwargs)\n",
        "\n",
        "  return log.history[\"loss\"], log.history[\"binary_accuracy\"], log.history[\"val_loss\"], log.history[\"val_binary_accuracy\"]"
      ],
      "metadata": {
        "id": "-ShwqBsOb5Dd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if device == \"GPU\":\n",
        "  stop_early = keras.callbacks.EarlyStopping(monitor='val_binary_accuracy',\n",
        "                                            patience=20,\n",
        "                                            restore_best_weights=True,\n",
        "                                            min_delta=0.001)"
      ],
      "metadata": {
        "id": "r03OvT0zuBxZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if device == \"GPU\":\n",
        "  epochs = 100\n",
        "  classifier_model = create_classifier()\n",
        "  classifier_accuracy = train_classifier(classifier_model, raw_train_ds, raw_val_ds, callbacks=stop_early, epochs=epochs)\n",
        "  print(\"Accuracy max %0.1f %% reached at the epoch %d\" %(100*max(classifier_accuracy[3]), np.argmax(classifier_accuracy[3])+1))\n",
        "  classifier_model.save('07_classification_text.keras')"
      ],
      "metadata": {
        "id": "aI4QvL6avN-T",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c95f9b6f-5e13-4295-c025-520bad34cea6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/100\n",
            "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 13ms/step - binary_accuracy: 0.5534 - loss: 0.6886 - val_binary_accuracy: 0.6126 - val_loss: 0.6760\n",
            "Epoch 2/100\n",
            "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 11ms/step - binary_accuracy: 0.5956 - loss: 0.6743 - val_binary_accuracy: 0.6394 - val_loss: 0.6592\n",
            "Epoch 3/100\n",
            "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 12ms/step - binary_accuracy: 0.6234 - loss: 0.6600 - val_binary_accuracy: 0.6574 - val_loss: 0.6460\n",
            "Epoch 4/100\n",
            "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 12ms/step - binary_accuracy: 0.6334 - loss: 0.6500 - val_binary_accuracy: 0.6716 - val_loss: 0.6364\n",
            "Epoch 5/100\n",
            "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 11ms/step - binary_accuracy: 0.6427 - loss: 0.6443 - val_binary_accuracy: 0.6820 - val_loss: 0.6284\n",
            "Epoch 6/100\n",
            "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 13ms/step - binary_accuracy: 0.6487 - loss: 0.6387 - val_binary_accuracy: 0.6906 - val_loss: 0.6230\n",
            "Epoch 7/100\n",
            "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 10ms/step - binary_accuracy: 0.6552 - loss: 0.6340 - val_binary_accuracy: 0.6990 - val_loss: 0.6170\n",
            "Epoch 8/100\n",
            "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 12ms/step - binary_accuracy: 0.6604 - loss: 0.6310 - val_binary_accuracy: 0.7050 - val_loss: 0.6120\n",
            "Epoch 9/100\n",
            "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 13ms/step - binary_accuracy: 0.6668 - loss: 0.6250 - val_binary_accuracy: 0.7124 - val_loss: 0.6066\n",
            "Epoch 10/100\n",
            "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 19ms/step - binary_accuracy: 0.6768 - loss: 0.6199 - val_binary_accuracy: 0.7184 - val_loss: 0.6017\n",
            "Epoch 11/100\n",
            "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 10ms/step - binary_accuracy: 0.6723 - loss: 0.6174 - val_binary_accuracy: 0.7206 - val_loss: 0.5972\n",
            "Epoch 12/100\n",
            "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 13ms/step - binary_accuracy: 0.6794 - loss: 0.6120 - val_binary_accuracy: 0.7216 - val_loss: 0.5927\n",
            "Epoch 13/100\n",
            "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 10ms/step - binary_accuracy: 0.6825 - loss: 0.6074 - val_binary_accuracy: 0.7184 - val_loss: 0.5907\n",
            "Epoch 14/100\n",
            "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 13ms/step - binary_accuracy: 0.6893 - loss: 0.6033 - val_binary_accuracy: 0.7292 - val_loss: 0.5846\n",
            "Epoch 15/100\n",
            "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 10ms/step - binary_accuracy: 0.6926 - loss: 0.6022 - val_binary_accuracy: 0.7280 - val_loss: 0.5824\n",
            "Epoch 16/100\n",
            "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 13ms/step - binary_accuracy: 0.6940 - loss: 0.5993 - val_binary_accuracy: 0.7284 - val_loss: 0.5800\n",
            "Epoch 17/100\n",
            "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 11ms/step - binary_accuracy: 0.7042 - loss: 0.5945 - val_binary_accuracy: 0.7322 - val_loss: 0.5743\n",
            "Epoch 18/100\n",
            "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 11ms/step - binary_accuracy: 0.6998 - loss: 0.5937 - val_binary_accuracy: 0.7330 - val_loss: 0.5726\n",
            "Epoch 19/100\n",
            "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 14ms/step - binary_accuracy: 0.7061 - loss: 0.5879 - val_binary_accuracy: 0.7360 - val_loss: 0.5688\n",
            "Epoch 20/100\n",
            "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 11ms/step - binary_accuracy: 0.7114 - loss: 0.5846 - val_binary_accuracy: 0.7382 - val_loss: 0.5660\n",
            "Epoch 21/100\n",
            "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 13ms/step - binary_accuracy: 0.7061 - loss: 0.5863 - val_binary_accuracy: 0.7438 - val_loss: 0.5630\n",
            "Epoch 22/100\n",
            "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 11ms/step - binary_accuracy: 0.7125 - loss: 0.5806 - val_binary_accuracy: 0.7452 - val_loss: 0.5602\n",
            "Epoch 23/100\n",
            "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 14ms/step - binary_accuracy: 0.7152 - loss: 0.5765 - val_binary_accuracy: 0.7472 - val_loss: 0.5576\n",
            "Epoch 24/100\n",
            "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 11ms/step - binary_accuracy: 0.7144 - loss: 0.5750 - val_binary_accuracy: 0.7514 - val_loss: 0.5538\n",
            "Epoch 25/100\n",
            "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 13ms/step - binary_accuracy: 0.7202 - loss: 0.5723 - val_binary_accuracy: 0.7498 - val_loss: 0.5525\n",
            "Epoch 26/100\n",
            "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 13ms/step - binary_accuracy: 0.7166 - loss: 0.5705 - val_binary_accuracy: 0.7508 - val_loss: 0.5500\n",
            "Epoch 27/100\n",
            "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 11ms/step - binary_accuracy: 0.7280 - loss: 0.5676 - val_binary_accuracy: 0.7554 - val_loss: 0.5466\n",
            "Epoch 28/100\n",
            "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 11ms/step - binary_accuracy: 0.7156 - loss: 0.5682 - val_binary_accuracy: 0.7544 - val_loss: 0.5447\n",
            "Epoch 29/100\n",
            "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 13ms/step - binary_accuracy: 0.7259 - loss: 0.5647 - val_binary_accuracy: 0.7564 - val_loss: 0.5428\n",
            "Epoch 30/100\n",
            "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 10ms/step - binary_accuracy: 0.7262 - loss: 0.5620 - val_binary_accuracy: 0.7602 - val_loss: 0.5406\n",
            "Epoch 31/100\n",
            "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 11ms/step - binary_accuracy: 0.7252 - loss: 0.5620 - val_binary_accuracy: 0.7624 - val_loss: 0.5373\n",
            "Epoch 32/100\n",
            "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 13ms/step - binary_accuracy: 0.7393 - loss: 0.5517 - val_binary_accuracy: 0.7622 - val_loss: 0.5354\n",
            "Epoch 33/100\n",
            "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 10ms/step - binary_accuracy: 0.7337 - loss: 0.5521 - val_binary_accuracy: 0.7652 - val_loss: 0.5324\n",
            "Epoch 34/100\n",
            "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 13ms/step - binary_accuracy: 0.7290 - loss: 0.5566 - val_binary_accuracy: 0.7648 - val_loss: 0.5310\n",
            "Epoch 35/100\n",
            "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 10ms/step - binary_accuracy: 0.7291 - loss: 0.5506 - val_binary_accuracy: 0.7662 - val_loss: 0.5290\n",
            "Epoch 36/100\n",
            "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 13ms/step - binary_accuracy: 0.7366 - loss: 0.5457 - val_binary_accuracy: 0.7682 - val_loss: 0.5268\n",
            "Epoch 37/100\n",
            "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 12ms/step - binary_accuracy: 0.7316 - loss: 0.5496 - val_binary_accuracy: 0.7682 - val_loss: 0.5264\n",
            "Epoch 38/100\n",
            "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 11ms/step - binary_accuracy: 0.7371 - loss: 0.5454 - val_binary_accuracy: 0.7686 - val_loss: 0.5234\n",
            "Epoch 39/100\n",
            "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 13ms/step - binary_accuracy: 0.7373 - loss: 0.5443 - val_binary_accuracy: 0.7704 - val_loss: 0.5219\n",
            "Epoch 40/100\n",
            "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 12ms/step - binary_accuracy: 0.7381 - loss: 0.5422 - val_binary_accuracy: 0.7734 - val_loss: 0.5200\n",
            "Epoch 41/100\n",
            "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 12ms/step - binary_accuracy: 0.7472 - loss: 0.5376 - val_binary_accuracy: 0.7730 - val_loss: 0.5182\n",
            "Epoch 42/100\n",
            "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 13ms/step - binary_accuracy: 0.7399 - loss: 0.5406 - val_binary_accuracy: 0.7740 - val_loss: 0.5165\n",
            "Epoch 43/100\n",
            "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 15ms/step - binary_accuracy: 0.7410 - loss: 0.5398 - val_binary_accuracy: 0.7746 - val_loss: 0.5150\n",
            "Epoch 44/100\n",
            "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 10ms/step - binary_accuracy: 0.7374 - loss: 0.5408 - val_binary_accuracy: 0.7748 - val_loss: 0.5142\n",
            "Epoch 45/100\n",
            "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 13ms/step - binary_accuracy: 0.7371 - loss: 0.5369 - val_binary_accuracy: 0.7766 - val_loss: 0.5128\n",
            "Epoch 46/100\n",
            "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 11ms/step - binary_accuracy: 0.7421 - loss: 0.5357 - val_binary_accuracy: 0.7772 - val_loss: 0.5114\n",
            "Epoch 47/100\n",
            "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 13ms/step - binary_accuracy: 0.7423 - loss: 0.5298 - val_binary_accuracy: 0.7760 - val_loss: 0.5105\n",
            "Epoch 48/100\n",
            "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 11ms/step - binary_accuracy: 0.7462 - loss: 0.5318 - val_binary_accuracy: 0.7788 - val_loss: 0.5088\n",
            "Epoch 49/100\n",
            "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 11ms/step - binary_accuracy: 0.7467 - loss: 0.5300 - val_binary_accuracy: 0.7788 - val_loss: 0.5077\n",
            "Epoch 50/100\n",
            "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 13ms/step - binary_accuracy: 0.7451 - loss: 0.5267 - val_binary_accuracy: 0.7792 - val_loss: 0.5063\n",
            "Epoch 51/100\n",
            "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 12ms/step - binary_accuracy: 0.7487 - loss: 0.5264 - val_binary_accuracy: 0.7820 - val_loss: 0.5048\n",
            "Epoch 52/100\n",
            "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 12ms/step - binary_accuracy: 0.7468 - loss: 0.5260 - val_binary_accuracy: 0.7814 - val_loss: 0.5039\n",
            "Epoch 53/100\n",
            "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 13ms/step - binary_accuracy: 0.7482 - loss: 0.5271 - val_binary_accuracy: 0.7822 - val_loss: 0.5031\n",
            "Epoch 54/100\n",
            "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 11ms/step - binary_accuracy: 0.7486 - loss: 0.5256 - val_binary_accuracy: 0.7816 - val_loss: 0.5024\n",
            "Epoch 55/100\n",
            "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 14ms/step - binary_accuracy: 0.7519 - loss: 0.5237 - val_binary_accuracy: 0.7830 - val_loss: 0.5013\n",
            "Epoch 56/100\n",
            "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 11ms/step - binary_accuracy: 0.7508 - loss: 0.5281 - val_binary_accuracy: 0.7828 - val_loss: 0.5015\n",
            "Epoch 57/100\n",
            "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 13ms/step - binary_accuracy: 0.7425 - loss: 0.5304 - val_binary_accuracy: 0.7846 - val_loss: 0.5003\n",
            "Epoch 58/100\n",
            "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 13ms/step - binary_accuracy: 0.7501 - loss: 0.5241 - val_binary_accuracy: 0.7844 - val_loss: 0.4992\n",
            "Epoch 59/100\n",
            "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 11ms/step - binary_accuracy: 0.7459 - loss: 0.5277 - val_binary_accuracy: 0.7850 - val_loss: 0.4993\n",
            "Epoch 60/100\n",
            "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 13ms/step - binary_accuracy: 0.7540 - loss: 0.5244 - val_binary_accuracy: 0.7876 - val_loss: 0.4977\n",
            "Epoch 61/100\n",
            "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 11ms/step - binary_accuracy: 0.7567 - loss: 0.5182 - val_binary_accuracy: 0.7884 - val_loss: 0.4964\n",
            "Epoch 62/100\n",
            "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 13ms/step - binary_accuracy: 0.7485 - loss: 0.5224 - val_binary_accuracy: 0.7860 - val_loss: 0.4964\n",
            "Epoch 63/100\n",
            "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 11ms/step - binary_accuracy: 0.7466 - loss: 0.5197 - val_binary_accuracy: 0.7894 - val_loss: 0.4943\n",
            "Epoch 64/100\n",
            "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 14ms/step - binary_accuracy: 0.7470 - loss: 0.5229 - val_binary_accuracy: 0.7886 - val_loss: 0.4950\n",
            "Epoch 65/100\n",
            "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 13ms/step - binary_accuracy: 0.7556 - loss: 0.5146 - val_binary_accuracy: 0.7894 - val_loss: 0.4932\n",
            "Epoch 66/100\n",
            "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 11ms/step - binary_accuracy: 0.7565 - loss: 0.5177 - val_binary_accuracy: 0.7904 - val_loss: 0.4931\n",
            "Epoch 67/100\n",
            "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 13ms/step - binary_accuracy: 0.7552 - loss: 0.5164 - val_binary_accuracy: 0.7898 - val_loss: 0.4932\n",
            "Epoch 68/100\n",
            "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 11ms/step - binary_accuracy: 0.7529 - loss: 0.5200 - val_binary_accuracy: 0.7896 - val_loss: 0.4934\n",
            "Epoch 69/100\n",
            "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 13ms/step - binary_accuracy: 0.7525 - loss: 0.5163 - val_binary_accuracy: 0.7892 - val_loss: 0.4916\n",
            "Epoch 70/100\n",
            "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 11ms/step - binary_accuracy: 0.7523 - loss: 0.5151 - val_binary_accuracy: 0.7908 - val_loss: 0.4904\n",
            "Epoch 71/100\n",
            "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 11ms/step - binary_accuracy: 0.7562 - loss: 0.5124 - val_binary_accuracy: 0.7916 - val_loss: 0.4898\n",
            "Epoch 72/100\n",
            "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 12ms/step - binary_accuracy: 0.7552 - loss: 0.5163 - val_binary_accuracy: 0.7914 - val_loss: 0.4897\n",
            "Epoch 73/100\n",
            "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 13ms/step - binary_accuracy: 0.7589 - loss: 0.5126 - val_binary_accuracy: 0.7920 - val_loss: 0.4890\n",
            "Epoch 74/100\n",
            "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 11ms/step - binary_accuracy: 0.7599 - loss: 0.5074 - val_binary_accuracy: 0.7916 - val_loss: 0.4881\n",
            "Epoch 75/100\n",
            "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 13ms/step - binary_accuracy: 0.7577 - loss: 0.5152 - val_binary_accuracy: 0.7908 - val_loss: 0.4886\n",
            "Epoch 76/100\n",
            "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 12ms/step - binary_accuracy: 0.7607 - loss: 0.5140 - val_binary_accuracy: 0.7904 - val_loss: 0.4883\n",
            "Epoch 77/100\n",
            "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 13ms/step - binary_accuracy: 0.7538 - loss: 0.5136 - val_binary_accuracy: 0.7912 - val_loss: 0.4875\n",
            "Epoch 78/100\n",
            "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 12ms/step - binary_accuracy: 0.7514 - loss: 0.5154 - val_binary_accuracy: 0.7912 - val_loss: 0.4868\n",
            "Epoch 79/100\n",
            "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 13ms/step - binary_accuracy: 0.7548 - loss: 0.5133 - val_binary_accuracy: 0.7902 - val_loss: 0.4871\n",
            "Epoch 80/100\n",
            "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 11ms/step - binary_accuracy: 0.7537 - loss: 0.5166 - val_binary_accuracy: 0.7900 - val_loss: 0.4874\n",
            "Epoch 81/100\n",
            "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 14ms/step - binary_accuracy: 0.7573 - loss: 0.5112 - val_binary_accuracy: 0.7896 - val_loss: 0.4871\n",
            "Epoch 82/100\n",
            "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 11ms/step - binary_accuracy: 0.7606 - loss: 0.5111 - val_binary_accuracy: 0.7904 - val_loss: 0.4856\n",
            "Epoch 83/100\n",
            "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 26ms/step - binary_accuracy: 0.7602 - loss: 0.5087 - val_binary_accuracy: 0.7908 - val_loss: 0.4846\n",
            "Epoch 84/100\n",
            "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 15ms/step - binary_accuracy: 0.7562 - loss: 0.5145 - val_binary_accuracy: 0.7900 - val_loss: 0.4847\n",
            "Epoch 85/100\n",
            "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 12ms/step - binary_accuracy: 0.7640 - loss: 0.5087 - val_binary_accuracy: 0.7914 - val_loss: 0.4842\n",
            "Epoch 86/100\n",
            "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 11ms/step - binary_accuracy: 0.7570 - loss: 0.5101 - val_binary_accuracy: 0.7918 - val_loss: 0.4839\n",
            "Epoch 87/100\n",
            "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 13ms/step - binary_accuracy: 0.7581 - loss: 0.5063 - val_binary_accuracy: 0.7908 - val_loss: 0.4837\n",
            "Epoch 88/100\n",
            "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 13ms/step - binary_accuracy: 0.7605 - loss: 0.5098 - val_binary_accuracy: 0.7916 - val_loss: 0.4829\n",
            "Epoch 89/100\n",
            "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 11ms/step - binary_accuracy: 0.7563 - loss: 0.5113 - val_binary_accuracy: 0.7918 - val_loss: 0.4831\n",
            "Epoch 90/100\n",
            "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 14ms/step - binary_accuracy: 0.7578 - loss: 0.5122 - val_binary_accuracy: 0.7924 - val_loss: 0.4823\n",
            "Epoch 91/100\n",
            "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 11ms/step - binary_accuracy: 0.7623 - loss: 0.5069 - val_binary_accuracy: 0.7930 - val_loss: 0.4815\n",
            "Epoch 92/100\n",
            "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 13ms/step - binary_accuracy: 0.7638 - loss: 0.5075 - val_binary_accuracy: 0.7944 - val_loss: 0.4814\n",
            "Epoch 93/100\n",
            "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 11ms/step - binary_accuracy: 0.7590 - loss: 0.5106 - val_binary_accuracy: 0.7932 - val_loss: 0.4819\n",
            "Epoch 94/100\n",
            "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 13ms/step - binary_accuracy: 0.7637 - loss: 0.5051 - val_binary_accuracy: 0.7936 - val_loss: 0.4813\n",
            "Epoch 95/100\n",
            "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 13ms/step - binary_accuracy: 0.7570 - loss: 0.5104 - val_binary_accuracy: 0.7946 - val_loss: 0.4808\n",
            "Epoch 96/100\n",
            "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 12ms/step - binary_accuracy: 0.7657 - loss: 0.5029 - val_binary_accuracy: 0.7964 - val_loss: 0.4801\n",
            "Epoch 97/100\n",
            "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 13ms/step - binary_accuracy: 0.7588 - loss: 0.5085 - val_binary_accuracy: 0.7956 - val_loss: 0.4798\n",
            "Epoch 98/100\n",
            "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 11ms/step - binary_accuracy: 0.7626 - loss: 0.5038 - val_binary_accuracy: 0.7950 - val_loss: 0.4795\n",
            "Epoch 99/100\n",
            "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 13ms/step - binary_accuracy: 0.7567 - loss: 0.5066 - val_binary_accuracy: 0.7972 - val_loss: 0.4790\n",
            "Epoch 100/100\n",
            "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 12ms/step - binary_accuracy: 0.7619 - loss: 0.5078 - val_binary_accuracy: 0.7968 - val_loss: 0.4791\n",
            "Accuracy max 79.7 % reached at the epoch 99\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 4.3. Evaluation and predictions"
      ],
      "metadata": {
        "id": "B_NpeYlfcWR5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_predictions(dataset):\n",
        "  loss_test, accuracy_test = classifier_model.evaluate(dataset, verbose=0)\n",
        "  predictions = classifier_model.predict(dataset, verbose=0)\n",
        "  print(\"Test loss value %0.1f \\nTest accuracy: %0.1f %%\" %(loss_test, 100*accuracy_test))\n",
        "  return predictions"
      ],
      "metadata": {
        "id": "pWv3x_2tO6L4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "predictions = get_predictions(raw_test_ds)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YpbdyqG2O_N6",
        "outputId": "5376a2cf-4be2-454c-cacd-28971ed42b96"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test loss value 0.5 \n",
            "Test accuracy: 80.0 %\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "plot_evaluation(raw_test_ds, predictions)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 591
        },
        "id": "eXDOnty4PUxd",
        "outputId": "f31deff7-8666-4a27-f874-93021dff1ecf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "############################\n",
            "Evaluation on the test set: \n",
            "############################\n",
            "Accuracy...0.8002\n",
            "Recall.....0.8268\n",
            "F1-score...0.8054\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1200x500 with 2 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "\n",
        "\n",
        "## **5. Embedding space exploration**"
      ],
      "metadata": {
        "id": "O2ayOZiwYfeN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 5.1. Embedding dictionary"
      ],
      "metadata": {
        "id": "h2ApzsXuYoie"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "if device == \"GPU\":\n",
        "  vocab = vectorize_layer.get_vocabulary()\n",
        "  dictionary_embedding = {vocab[i]:weights[i] for i in range(len(vocab))} # build the embedding dictionary\n",
        "\n",
        "  with open('vocab.pkl', 'wb') as f:\n",
        "    pickle.dump(vocab, f)\n",
        "\n",
        "  with open('dictionary_embedding.pkl', 'wb') as f:\n",
        "    pickle.dump(dictionary_embedding, f)"
      ],
      "metadata": {
        "id": "KEAQX3iF4NDi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dictionary_embedding[\"money\"]"
      ],
      "metadata": {
        "id": "UF-ehK1Lie7m",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0f24c975-14bd-4287-f1f4-89e1c9b4cd10"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([-0.49468917, -0.24695764, -0.35080495,  0.17630659,  0.40587196,\n",
              "        0.45379823,  0.42825904, -0.16315092,  0.661346  ,  0.2372665 ,\n",
              "        0.7107482 , -0.30229184,  0.40659153,  0.7007456 ,  0.20098907,\n",
              "        0.7558826 ], dtype=float32)"
            ]
          },
          "metadata": {},
          "execution_count": 63
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 5.2. Similarities and analogies"
      ],
      "metadata": {
        "id": "jkHFz7FSYv5R"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# get the distance between two elements in the embedding space\n",
        "\n",
        "def get_distance(token1, token2):\n",
        "  p1 = dictionary_embedding[token1]\n",
        "  p2 = dictionary_embedding[token2]\n",
        "  distance = np.linalg.norm(p2-p1)\n",
        "  return distance\n",
        "\n",
        "# get the cosinus similarity between two elements in the embedding space\n",
        "\n",
        "def get_cosinus_similarity(token1, token2):\n",
        "  p1 = dictionary_embedding[token1]\n",
        "  p2 = dictionary_embedding[token2]\n",
        "  dot_product = np.dot(p1, p2)\n",
        "  magnitude_1 = np.linalg.norm(p1)\n",
        "  magnitude_2 = np.linalg.norm(p2)\n",
        "  cosine_sim = dot_product / (magnitude_1 * magnitude_2)\n",
        "  return cosine_sim"
      ],
      "metadata": {
        "id": "D32RiQSyihOX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# get elements closest to a specific element in the embedding space\n",
        "\n",
        "def get_synomym(token, n, used_distance=True):\n",
        "  p1 = dictionary_embedding[token]\n",
        "  candidate_list = {} # stores n synonyms\n",
        "  for i in range(1, len(vocab)): # only the first 1000 words of the embedding dictionary are searched\n",
        "    token_candidate = vocab[i]\n",
        "    if used_distance == True: candidate_list[token_candidate] = get_distance(token, token_candidate)\n",
        "    else: candidate_list[token_candidate] = get_cosinus_similarity(token, token_candidate)\n",
        "\n",
        "  sorted_items = sorted(candidate_list.items(), key=lambda item: item[1])\n",
        "\n",
        "  if used_distance == True: synonym_list = sorted_items[1:n+1]\n",
        "  else: synonym_list = sorted_items[-(n+1):-1]\n",
        "  words = [(item[0], item[1]) for item in synonym_list]\n",
        "  return print(words)"
      ],
      "metadata": {
        "id": "FNWrbXIUiuI7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "word = \"terrible\"\n",
        "nb = 10\n",
        "get_synomym(word, nb, used_distance=False)"
      ],
      "metadata": {
        "id": "8sUp8U2FiwUP",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a6611861-9029-4faa-e217-603f76dad06c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[('ridiculous', 0.8171861), ('poor', 0.81820154), ('pathetic', 0.81890875), ('minimal', 0.8197048), ('atrocious', 0.826706), ('uninspired', 0.8299742), ('appalling', 0.8299839), ('laughable', 0.83978975), ('unfunny', 0.8417924), ('horrible', 0.9106703)]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def get_analogy(vector, n):\n",
        "  candidate_list = {} # stores n synonyms\n",
        "  for i in range(1, len(vocab)):\n",
        "    token_candidate = vocab[i]\n",
        "    vector_candidate = dictionary_embedding[token_candidate]\n",
        "    candidate_list[token_candidate] = sum(abs(vector - vector_candidate))\n",
        "\n",
        "  sorted_items = sorted(candidate_list.items(), key=lambda item: item[1])\n",
        "  synonym_list = sorted_items[0:n]\n",
        "  words = [item[0] for item in synonym_list]\n",
        "  print(words)"
      ],
      "metadata": {
        "id": "FlzOsPSGRUmY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "analogy = dictionary_embedding[\"funny\"] - dictionary_embedding[\"nice\"] + dictionary_embedding[\"terrible\"]\n",
        "get_analogy(analogy, 5)"
      ],
      "metadata": {
        "id": "TvO-QKgeRWPa",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fcd1e3f2-4048-47b0-f879-7fa7bcb8a91b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['boring', 'unbearable', 'unwatchable', 'unrealistic', 'dull']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "\n",
        "\n",
        "## **6. References**\n",
        "\n",
        "| | | | | |\n",
        "|------|------|------|------|------|\n",
        "| Index | Title | Author(s) | Type | Comments |\n",
        "|[[1]](https://aclanthology.org/P11-1015.pdf) | IMDB dataset | Andrew L. Maas & al | dataset & paper | - |\n",
        "|[[2]](https://www.tensorflow.org/tutorials/keras/text_classification) | Basic text classification | TensorFlow | dataset | - |\n",
        "|[[3]](https://www.tensorflow.org/guide/data_performance) | Better performance with the tf.data API | TensorFlow | Tutoriels | - |\n",
        "|[[4]](https://www.cs.toronto.edu/~lczhang/360/lec/w05/w2v.html) | Word2Vec and GloVe Vectors | Toronto university | website | - |"
      ],
      "metadata": {
        "id": "iA4CnxdVY7OH"
      }
    }
  ]
}